import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
import random
import os
np.random.seed(42)
random.seed(42)
os.environ['PYTHONHASHSEED'] = '42'

# æ ¸å¿ƒåº“å¯¼å…¥
from sklearn.model_selection import train_test_split, KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

def mean_absolute_percentage_error(y_true, y_pred):
    """è®¡ç®—å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·® (MAPE) - æ¥è‡ªGithubé¡¹ç›®"""
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    # é¿å…é™¤é›¶é”™è¯¯
    mask = y_true != 0
    if mask.sum() == 0:
        return 0.0
    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100

def calculate_competition_wmae(y_true_dict, y_pred_dict, task_names):
    """
    è®¡ç®—ç«èµ›æ ‡å‡†çš„åŠ æƒå¹³å‡ç»å¯¹è¯¯å·® (wMAE)

    å…¬å¼: wMAE = (1/|X|) * Î£_X Î£_iâˆˆI(X) wi * |Å·i(X) - yi(X)|

    å…¶ä¸­æƒé‡ wi = (1/ri) * (K * âˆš(1/ni)) / (Î£_j=1^K âˆš(1/nj))

    å‚æ•°:
    - y_true_dict: {task: y_true_array} çœŸå®å€¼å­—å…¸
    - y_pred_dict: {task: y_pred_array} é¢„æµ‹å€¼å­—å…¸
    - task_names: ä»»åŠ¡åç§°åˆ—è¡¨

    è¿”å›:
    - wmae: åŠ æƒå¹³å‡ç»å¯¹è¯¯å·®
    - weights: å„ä»»åŠ¡æƒé‡å­—å…¸
    """
    K = len(task_names)  # æ€»ä»»åŠ¡æ•°
    weights = {}
    mae_values = {}

    # ç¬¬ä¸€æ­¥ï¼šè®¡ç®—æ¯ä¸ªä»»åŠ¡çš„åŸºç¡€ç»Ÿè®¡
    task_stats = {}
    for task in task_names:
        if task in y_true_dict and task in y_pred_dict:
            y_true = np.array(y_true_dict[task])
            y_pred = np.array(y_pred_dict[task])

            # ç§»é™¤NaNå€¼
            mask = ~(np.isnan(y_true) | np.isnan(y_pred))
            y_true_clean = y_true[mask]
            y_pred_clean = y_pred[mask]

            if len(y_true_clean) > 0:
                # ni: å¯ç”¨æ ·æœ¬æ•°
                ni = len(y_true_clean)

                # ri: å±æ€§å€¼èŒƒå›´ (åŸºäºçœŸå®å€¼)
                ri = np.max(y_true_clean) - np.min(y_true_clean)
                if ri == 0:
                    ri = 1.0  # é¿å…é™¤é›¶

                # MAE
                mae = np.mean(np.abs(y_true_clean - y_pred_clean))

                task_stats[task] = {
                    'ni': ni,
                    'ri': ri,
                    'mae': mae,
                    'y_true': y_true_clean,
                    'y_pred': y_pred_clean
                }

    if not task_stats:
        return 999.0, {}

    # ç¬¬äºŒæ­¥ï¼šè®¡ç®—æƒé‡
    # æ ¹æ®ç«èµ›å…¬å¼ï¼Œæƒé‡åº”è¯¥ç¡®ä¿æ‰€æœ‰Kä¸ªä»»åŠ¡çš„æƒé‡å’Œä¸ºK
    # å…ˆè®¡ç®—æœªæ ‡å‡†åŒ–çš„æƒé‡
    unnormalized_weights = {}
    for task, stats in task_stats.items():
        # ç¬¬ä¸€éƒ¨åˆ†ï¼š1/ri (å°ºåº¦æ ‡å‡†åŒ–)
        scale_factor = 1.0 / stats['ri']
        # ç¬¬äºŒéƒ¨åˆ†ï¼šâˆš(1/ni) (é€†å¹³æ–¹æ ¹ç¼©æ”¾)
        inverse_sqrt_scaling = np.sqrt(1.0 / stats['ni'])

        unnormalized_weights[task] = scale_factor * inverse_sqrt_scaling

    # è®¡ç®—æ ‡å‡†åŒ–å› å­ï¼Œä½¿å¾—æƒé‡å’Œä¸ºK
    total_unnormalized = sum(unnormalized_weights.values())
    normalization_factor = K / total_unnormalized

    # åº”ç”¨æ ‡å‡†åŒ–
    for task in unnormalized_weights:
        weights[task] = unnormalized_weights[task] * normalization_factor
        mae_values[task] = task_stats[task]['mae']

    # ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—åŠ æƒMAE
    # wMAE = Î£_i wi * MAE_i
    wmae = sum(weights[task] * mae_values[task] for task in weights.keys())

    return wmae, weights
from sklearn.feature_selection import VarianceThreshold
from sklearn.mixture import GaussianMixture

# åˆ†å­å¤„ç†åº“
from rdkit import Chem
from rdkit.Chem import AllChem, Descriptors, rdMolDescriptors, MACCSkeys, Fragments, Lipinski
from rdkit.Chem import rdmolops
from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator, GetAtomPairGenerator, GetTopologicalTorsionGenerator
from rdkit import RDLogger
RDLogger.DisableLog('rdApp.*')

# å›¾è®ºåº“
import networkx as nx

# æœºå™¨å­¦ä¹ åº“
import xgboost as xgb
from xgboost import XGBRegressor
import pickle
import joblib

# å°è¯•å¯¼å…¥torch_moleculeåº“
try:
    from torch_molecule import LSTMMolecularPredictor, GNNMolecularPredictor
    from torch_molecule.utils.search import ParameterType, ParameterSpec
    TORCH_MOLECULE_AVAILABLE = True
    print("âœ… torch_moleculeåº“å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¸“ä¸šåˆ†å­é¢„æµ‹æ¨¡å‹")
except ImportError:
    TORCH_MOLECULE_AVAILABLE = False
    print("âš ï¸ torch_moleculeåº“ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•")

class UltimateSolution:
    """ç»ˆæè§£å†³æ–¹æ¡ˆ - æ•´åˆæ‰€æœ‰æœ€ä½³å®è·µ"""
    
    def __init__(self, use_torch_molecule=True, fast_mode=False, model_path=None, use_deep_learning=False, use_saved_models=False):
        self.use_torch_molecule = use_torch_molecule and TORCH_MOLECULE_AVAILABLE
        self.use_deep_learning = use_deep_learning  # æ·±åº¦å­¦ä¹ å¼€å…³ï¼Œé»˜è®¤å…³é—­
        self.fast_mode = fast_mode  # å¿«é€Ÿæ¨¡å¼ï¼Œå‡å°‘è®­ç»ƒæ—¶é—´
        self.use_saved_models = use_saved_models  # æ˜¯å¦ä½¿ç”¨å·²ä¿å­˜çš„æ¨¡å‹ï¼Œé»˜è®¤å…³é—­
        self.model_path = model_path  # è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„
        self.task_names = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']
        self.models = {}
        self.scalers = {}
        self.feature_selectors = {}  # ç¡®ä¿åˆå§‹åŒ–
        self.frequency_selectors = {}  # é¢‘æ¬¡ç‰¹å¾é€‰æ‹©å™¨

        # Githubé¡¹ç›®å¯å‘çš„ç‰¹å¾é€‰æ‹© - åŸºäºé¢‘æ¬¡ç»Ÿè®¡
        self.feature_frequency_stats = {}  # ç‰¹å¾é¢‘æ¬¡ç»Ÿè®¡
        self.frequency_threshold = 6400  # Githubé¡¹ç›®çš„NumberOfZeroé˜ˆå€¼

        # GitHubä¸“ä¸šç‰¹å¾å·¥ç¨‹æ€æƒ³ï¼ˆä¸ä¾èµ–å¤–éƒ¨æ–‡ä»¶ï¼‰
        self.use_github_features = False  # ğŸ”„ æš‚æ—¶ç¦ç”¨GitHubç‰¹å¾å·¥ç¨‹ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•æµ‹è¯•
        self.github_style_features = False  # ä½¿ç”¨ä¼ ç»Ÿç‰¹å¾å·¥ç¨‹

        # GitHubç‰¹å¾å·¥ç¨‹å‚æ•°ï¼ˆåŸºäºè®ºæ–‡å’Œæœ€ä½³å®è·µï¼‰
        self.morgan_radius = 3  # GitHubé¡¹ç›®ä½¿ç”¨radius=3
        self.frequency_threshold = 6400  # GitHubçš„ç‰¹å¾é€‰æ‹©é˜ˆå€¼
        self.target_features = 124  # GitHubä¼˜åŒ–åçš„ç‰¹å¾æ•°é‡

        # è®¾ç½®GitHubé£æ ¼ç‰¹å¾å·¥ç¨‹
        self._setup_github_style_features()

        # GitHubç›¸å…³çŠ¶æ€æ ‡å¿—ï¼ˆå…¼å®¹æ€§ï¼‰
        self.github_data_loaded = False  # ä¸å†ä¾èµ–å¤–éƒ¨æ•°æ®æ–‡ä»¶
        self.github_components_loaded = False  # ä¸å†ä¾èµ–å¤–éƒ¨ç»„ä»¶

        # ä»»åŠ¡æƒé‡ï¼ˆåŸºäºé‡è¦æ€§ï¼‰
        self.task_weights = {
            'Tg': 0.3,      # ç»ç’ƒåŒ–è½¬å˜æ¸©åº¦ - æœ€é‡è¦
            'FFV': 0.25,    # è‡ªç”±ä½“ç§¯åˆ†æ•°
            'Tc': 0.2,      # ä¸´ç•Œæ¸©åº¦
            'Density': 0.15, # å¯†åº¦
            'Rg': 0.1       # å›è½¬åŠå¾„
        }

        # å¿…éœ€æè¿°ç¬¦
        self.required_descriptors = {'graph_diameter','num_cycles','avg_shortest_path','MolWt', 'LogP', 'TPSA', 'RotatableBonds', 'NumAtoms'}

        # ä»»åŠ¡ç‰¹å®šç‰¹å¾è¿‡æ»¤å™¨
        self.task_filters = self._define_task_filters()

        # åˆå§‹åŒ–æ•°æ®åˆ†å‰²å˜é‡
        self.dev_train = None
        self.dev_val = None
        self.dev_test = None
        self.subtables = None
        self.test_df = None

        # GPUåŠ é€Ÿè®¾ç½®
        self._setup_gpu_acceleration()

        print(f"ğŸš€ ç»ˆæè§£å†³æ–¹æ¡ˆåˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨{'ä¸“ä¸šåˆ†å­é¢„æµ‹åº“' if self.use_torch_molecule else 'ä¼ ç»Ÿæœºå™¨å­¦ä¹ '}æ–¹æ³•")
        print(f"ğŸ§  æ·±åº¦å­¦ä¹ : {'å¯ç”¨' if self.use_deep_learning else 'ç¦ç”¨'}")
        print(f"ğŸ’¾ å·²ä¿å­˜æ¨¡å‹: {'å¯ç”¨' if self.use_saved_models else 'ç¦ç”¨ï¼ˆå¼ºåˆ¶é‡æ–°è®­ç»ƒï¼‰'}")
        if self.fast_mode:
            print("âš¡ å¿«é€Ÿæ¨¡å¼å·²å¯ç”¨ï¼Œå°†ä¼˜åŒ–è®­ç»ƒå‚æ•°ä»¥æé«˜æ•ˆç‡")
        if self.use_github_features and self.github_data_loaded:
            print("ğŸ¯ GitHubä¸“ä¸šç‰¹å¾å·¥ç¨‹å·²å¯ç”¨")

    def _setup_github_style_features(self):
        """è®¾ç½®GitHubé£æ ¼çš„ç‰¹å¾å·¥ç¨‹ï¼ˆä¸ä¾èµ–å¤–éƒ¨æ–‡ä»¶ï¼‰"""
        print("ğŸ¯ è®¾ç½®GitHubé£æ ¼çš„ä¸“ä¸šç‰¹å¾å·¥ç¨‹...")

        # GitHubç‰¹å¾å·¥ç¨‹çš„æ ¸å¿ƒæ€æƒ³ï¼š
        # 1. ä½¿ç”¨radius=3çš„MorganæŒ‡çº¹ï¼ˆæ¯”ä¼ ç»Ÿradius=2æ›´ä¸°å¯Œï¼‰
        # 2. æ™ºèƒ½ç‰¹å¾é€‰æ‹©ï¼ˆåŸºäºé¢‘æ¬¡å’Œæ–¹å·®ï¼‰
        # 3. å¤šå±‚æ¬¡ç‰¹å¾ç»„åˆ
        # 4. é’ˆå¯¹èšåˆç‰©çš„ä¸“é—¨ä¼˜åŒ–

        self.github_style_ready = True
        print("   âœ… GitHubé£æ ¼ç‰¹å¾å·¥ç¨‹å·²å‡†å¤‡å°±ç»ª")
        print("   ğŸ”¬ MorganæŒ‡çº¹: radius=3 (GitHubä¼˜åŒ–)")
        print("   ğŸ¯ æ™ºèƒ½ç‰¹å¾é€‰æ‹©: é¢‘æ¬¡+æ–¹å·®åŒé‡è¿‡æ»¤")
        print("   ğŸ§ª èšåˆç‰©ä¸“ç”¨: é’ˆå¯¹Tgç­‰ç‰©æ€§ä¼˜åŒ–")
        print("   ğŸ“Š ç›®æ ‡ç‰¹å¾æ•°: ~124 (GitHubæœ€ä½³å®è·µ)")

    def _extract_github_style_features(self, smiles_list, task_filter):
        """ä½¿ç”¨GitHubé£æ ¼çš„ä¸“ä¸šç‰¹å¾å·¥ç¨‹æ–¹æ³•ï¼ˆä¸ä¾èµ–å¤–éƒ¨æ–‡ä»¶ï¼‰"""
        print(f"ğŸ¯ ä½¿ç”¨GitHubé£æ ¼ä¸“ä¸šç‰¹å¾å·¥ç¨‹: {len(smiles_list)} ä¸ªSMILES")

        try:
            # è½¬æ¢ä¸ºåˆ†å­å¯¹è±¡
            molecules = []
            valid_smiles = []
            invalid_indices = []

            for i, smiles in enumerate(smiles_list):
                try:
                    mol = Chem.MolFromSmiles(smiles)
                    if mol is not None:
                        molecules.append(mol)
                        valid_smiles.append(smiles)
                    else:
                        invalid_indices.append(i)
                except:
                    invalid_indices.append(i)

            if len(molecules) == 0:
                print("   âŒ æ²¡æœ‰æœ‰æ•ˆçš„åˆ†å­")
                return np.array([]), np.array([]), [], list(range(len(smiles_list)))

            # === GitHubé£æ ¼ä¸“ä¸šç‰¹å¾å·¥ç¨‹æµç¨‹ ===

            # 1. ç”Ÿæˆé«˜è´¨é‡MorganæŒ‡çº¹ (radius=3, GitHubä¼˜åŒ–)
            print(f"   ğŸ”¬ ç”Ÿæˆé«˜è´¨é‡MorganæŒ‡çº¹ (radius=3)...")

            # ä½¿ç”¨å¤šç§åŠå¾„çš„MorganæŒ‡çº¹ç»„åˆ
            fingerprints_list = []

            # GitHubé£æ ¼: radius=3çš„MorganæŒ‡çº¹
            for mol in molecules:
                fp3 = AllChem.GetMorganFingerprintAsBitVect(mol, radius=3, nBits=1024)
                fp2 = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=512)
                fp1 = AllChem.GetMorganFingerprintAsBitVect(mol, radius=1, nBits=256)

                # ç»„åˆä¸åŒåŠå¾„çš„æŒ‡çº¹
                combined_fp = np.concatenate([
                    np.array(fp3),
                    np.array(fp2),
                    np.array(fp1)
                ])
                fingerprints_list.append(combined_fp)

            fingerprints_matrix = np.array(fingerprints_list)

            # 2. GitHubé£æ ¼çš„æ™ºèƒ½ç‰¹å¾é€‰æ‹©
            print(f"   ğŸ¯ åº”ç”¨GitHubé£æ ¼æ™ºèƒ½ç‰¹å¾é€‰æ‹©...")

            # 2a. é¢‘æ¬¡è¿‡æ»¤ (ç§»é™¤è¿‡äºç¨€ç–çš„ç‰¹å¾)
            feature_counts = np.sum(fingerprints_matrix > 0, axis=0)
            min_frequency = max(1, len(molecules) * 0.01)  # è‡³å°‘1%çš„åˆ†å­åŒ…å«è¯¥ç‰¹å¾
            max_frequency = len(molecules) * 0.95  # æœ€å¤š95%çš„åˆ†å­åŒ…å«è¯¥ç‰¹å¾

            frequency_mask = (feature_counts >= min_frequency) & (feature_counts <= max_frequency)
            selected_features = fingerprints_matrix[:, frequency_mask]

            # 2b. æ–¹å·®è¿‡æ»¤ (ç§»é™¤æ–¹å·®è¿‡å°çš„ç‰¹å¾)
            if selected_features.shape[1] > self.target_features:
                from sklearn.feature_selection import VarianceThreshold
                var_selector = VarianceThreshold(threshold=0.01)
                try:
                    selected_features = var_selector.fit_transform(selected_features)
                except:
                    pass  # å¦‚æœæ–¹å·®è¿‡æ»¤å¤±è´¥ï¼Œä¿æŒåŸç‰¹å¾

            # 2c. å¦‚æœç‰¹å¾ä»ç„¶å¤ªå¤šï¼Œä½¿ç”¨SelectKBest
            if selected_features.shape[1] > self.target_features:
                print(f"   ğŸ”§ åº”ç”¨SelectKBesté€‰æ‹©æœ€ä½³{self.target_features}ä¸ªç‰¹å¾...")
                # è¿™é‡Œæˆ‘ä»¬å…ˆä¿æŒæ‰€æœ‰ç‰¹å¾ï¼Œåœ¨è®­ç»ƒæ—¶å†è¿›è¡Œé€‰æ‹©

            print(f"   âœ… GitHubé£æ ¼ç‰¹å¾é€‰æ‹©: {fingerprints_matrix.shape[1]} â†’ {selected_features.shape[1]} ç‰¹å¾")
            print(f"   ğŸ“Š ç‰¹å¾å¯†åº¦: {np.count_nonzero(selected_features) / selected_features.size * 100:.1f}% éé›¶å…ƒç´ ")

            # 3. æ·»åŠ GitHubé£æ ¼çš„ç²¾é€‰åˆ†å­æè¿°ç¬¦
            print(f"   ğŸ§ª æ·»åŠ GitHubé£æ ¼ç²¾é€‰åˆ†å­æè¿°ç¬¦...")
            descriptors = []

            # GitHubé¡¹ç›®ä¸­éªŒè¯æœ‰æ•ˆçš„å…³é”®æè¿°ç¬¦
            key_descriptors = [
                'MolWt', 'MolLogP', 'NumHDonors', 'NumHAcceptors', 'TPSA',
                'NumRotatableBonds', 'NumAromaticRings', 'FractionCsp3',
                'HeavyAtomCount', 'RingCount', 'BertzCT', 'NumSaturatedRings',
                'NumAliphaticRings', 'NumHeterocycles', 'LabuteASA'
            ]

            for mol in molecules:
                desc_dict = {}

                # æ ¸å¿ƒæè¿°ç¬¦
                for desc_name in key_descriptors:
                    if hasattr(Descriptors, desc_name):
                        try:
                            desc_dict[desc_name] = getattr(Descriptors, desc_name)(mol)
                        except:
                            desc_dict[desc_name] = 0

                # ä»»åŠ¡ç‰¹å®šæè¿°ç¬¦
                for name, func in Descriptors.descList:
                    if name in task_filter and name not in desc_dict:
                        try:
                            desc_dict[name] = func(mol)
                        except:
                            desc_dict[name] = 0

                descriptors.append(desc_dict)

            descriptors_array = np.array(descriptors) if descriptors else np.array([])

            print(f"   âœ… GitHubé£æ ¼ç‰¹å¾å·¥ç¨‹å®Œæˆ!")
            print(f"   ğŸ“ˆ æœ€ç»ˆç‰¹å¾: {selected_features.shape[1]} æŒ‡çº¹ + {len(key_descriptors)} æè¿°ç¬¦")
            print(f"   ğŸ¯ ç‰¹å¾å·¥ç¨‹ç­–ç•¥: å¤šåŠå¾„MorganæŒ‡çº¹ + æ™ºèƒ½é€‰æ‹© + ç²¾é€‰æè¿°ç¬¦")

            return selected_features, descriptors_array, valid_smiles, invalid_indices

        except Exception as e:
            print(f"   âŒ GitHubé£æ ¼ç‰¹å¾æå–å¤±è´¥: {e}")
            print(f"   ğŸ”„ å›é€€åˆ°ä¼ ç»Ÿç‰¹å¾æå–æ–¹æ³•...")
            return self._extract_traditional_features_fallback(smiles_list, task_filter)

    def _extract_traditional_features_fallback(self, smiles_list, task_filter):
        """ä¼ ç»Ÿç‰¹å¾æå–æ–¹æ³•ï¼ˆå¤‡ç”¨ï¼‰"""
        print(f"ğŸ”§ ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•æå–ç‰¹å¾: {len(smiles_list)} ä¸ªSMILES")

        fingerprints = []
        descriptors = []
        valid_smiles = []
        invalid_indices = []

        for i, smiles in enumerate(smiles_list):
            try:
                mol = Chem.MolFromSmiles(smiles)
                if mol is None:
                    invalid_indices.append(i)
                    continue

                # MorganæŒ‡çº¹
                fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=128)
                fingerprints.append(list(fp))

                # æè¿°ç¬¦
                desc_dict = {}
                for name, func in Descriptors.descList:
                    if name in task_filter:
                        try:
                            desc_dict[name] = func(mol)
                        except:
                            desc_dict[name] = 0
                descriptors.append(desc_dict)

                valid_smiles.append(smiles)

            except Exception:
                invalid_indices.append(i)

        return np.array(fingerprints), np.array(descriptors), valid_smiles, invalid_indices
    
    def _setup_gpu_acceleration(self):
        """è®¾ç½®GPUåŠ é€Ÿ"""
        try:
            import torch
            if torch.cuda.is_available():
                self.device = torch.device('cuda')
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                print(f"âœ… GPUåŠ é€Ÿå·²å¯ç”¨: {gpu_name} ({gpu_memory:.1f} GB)")
                
                # GPUä¼˜åŒ–è®¾ç½®
                torch.backends.cudnn.benchmark = True  # ä¼˜åŒ–å·ç§¯æ€§èƒ½
                torch.backends.cudnn.deterministic = False  # å…è®¸éç¡®å®šæ€§ç®—æ³•ä»¥æå‡æ€§èƒ½
                
                # æ¸…ç†GPUå†…å­˜
                torch.cuda.empty_cache()
                
                # è®¾ç½®GPUå†…å­˜ç®¡ç†ï¼ˆä½¿ç”¨80%çš„GPUå†…å­˜ï¼‰
                if hasattr(torch.cuda, 'set_per_process_memory_fraction'):
                    torch.cuda.set_per_process_memory_fraction(0.8)
                    print("ğŸ”§ GPUå†…å­˜ä½¿ç”¨é™åˆ¶è®¾ç½®ä¸º80%")
                    
                print("ğŸš€ GPUä¼˜åŒ–è®¾ç½®å·²å¯ç”¨ï¼šbenchmark=True, å†…å­˜ä¼˜åŒ–")
            else:
                self.device = torch.device('cpu')
                print("âš ï¸ GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆå»ºè®®ä½¿ç”¨GPUä»¥è·å¾—æ›´å¥½æ€§èƒ½ï¼‰")
        except ImportError:
            self.device = torch.device('cpu')
            print("âš ï¸ PyTorchæœªå®‰è£…ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
    
    def _define_task_filters(self):
        """å®šä¹‰ä»»åŠ¡ç‰¹å®šçš„ç‰¹å¾è¿‡æ»¤å™¨"""
        return {
            'Tg': list(set([
                'BalabanJ','BertzCT','Chi1','Chi3n','Chi4n','EState_VSA4','EState_VSA8',
                'FpDensityMorgan3','HallKierAlpha','Kappa3','MaxAbsEStateIndex','MolLogP',
                'NumAmideBonds','NumHeteroatoms','NumHeterocycles','NumRotatableBonds',
                'PEOE_VSA14','Phi','RingCount','SMR_VSA1','SPS','SlogP_VSA1','SlogP_VSA5',
                'SlogP_VSA8','TPSA','VSA_EState1','VSA_EState4','VSA_EState6','VSA_EState7',
                'VSA_EState8','fr_C_O_noCOO','fr_NH1','fr_benzene','fr_bicyclic','fr_ether',
                'fr_unbrch_alkane',
                # Tgç‰¹å¼‚æ€§ç‰¹å¾ï¼ˆåŸºäºèšçƒ¯çƒƒTgçŸ¥è¯†ï¼‰
                'pp_units','pe_units','mol_wt','log_mol_wt','mol_wt_normalized',
                'rotatable_bonds','flexibility_ratio','aromatic_rings','aliphatic_rings',
                'methyl_groups','bulky_groups','h_bond_donors','h_bond_acceptors','h_bond_total',
                'polyolefin_type','expected_tg_range','double_bonds','triple_bonds',
                'asphericity','eccentricity','inertial_shape_factor',
                # èšåˆç‰©åˆ†ç±»ç‰¹å¼‚æ€§ç‰¹å¾ï¼ˆåŸºäºPolyInfoæ•°æ®åº“ï¼‰
                'polyolefin_score','ester_bonds','aromatic_ester_bonds','amide_bonds','aromatic_amide_bonds',
                'imide_groups','aromatic_imide_groups','ether_bonds','aromatic_ether_bonds',
                'acrylate_groups','methacrylate_groups','vinyl_groups','styrene_groups',
                'ester_ratio','amide_ratio','ether_ratio','aromatic_ratio',
                'rigid_segments','heterocycle_density'
            ]).union(self.required_descriptors)),

            'FFV': list(set([
                'AvgIpc','BalabanJ','BertzCT','Chi0','Chi0n','Chi0v','Chi1','Chi1n','Chi1v',
                'Chi2n','Chi2v','Chi3n','Chi3v','Chi4n','EState_VSA10','EState_VSA5',
                'EState_VSA7','EState_VSA8','EState_VSA9','ExactMolWt','FpDensityMorgan1',
                'FpDensityMorgan2','FpDensityMorgan3','FractionCSP3','HallKierAlpha',
                'HeavyAtomMolWt','Kappa1','Kappa2','Kappa3','MaxAbsEStateIndex',
                'MaxEStateIndex','MinEStateIndex','MolLogP','MolMR','MolWt','NHOHCount',
                'NOCount','NumAromaticHeterocycles','NumHAcceptors','NumHDonors',
                'NumHeterocycles','NumRotatableBonds','PEOE_VSA14','RingCount','SMR_VSA1',
                'SMR_VSA10','SMR_VSA3','SMR_VSA5','SMR_VSA6','SMR_VSA7','SMR_VSA9','SPS',
                'SlogP_VSA1','SlogP_VSA10','SlogP_VSA11','SlogP_VSA12','SlogP_VSA2',
                'SlogP_VSA3','SlogP_VSA4','SlogP_VSA5','SlogP_VSA6','SlogP_VSA7',
                'SlogP_VSA8','TPSA','VSA_EState1','VSA_EState10','VSA_EState2',
                'VSA_EState3','VSA_EState4','VSA_EState5','VSA_EState6','VSA_EState7',
                'VSA_EState8','VSA_EState9','fr_Ar_N','fr_C_O','fr_NH0','fr_NH1',
                'fr_aniline','fr_ether','fr_halogen','fr_thiophene'
            ]).union(self.required_descriptors)),

            'Tc': list(set([
                'BalabanJ','BertzCT','Chi0','EState_VSA5','ExactMolWt','FpDensityMorgan1',
                'FpDensityMorgan2','FpDensityMorgan3','HeavyAtomMolWt','MinEStateIndex',
                'MolWt','NumAtomStereoCenters','NumRotatableBonds','NumValenceElectrons',
                'SMR_VSA10','SMR_VSA7','SPS','SlogP_VSA6','SlogP_VSA8','VSA_EState1',
                'VSA_EState2','VSA_EState3','VSA_EState4','VSA_EState5','VSA_EState6',
                'VSA_EState7','VSA_EState8','VSA_EState9','fr_C_O','fr_NH0','fr_NH1',
                'fr_aniline','fr_ether','fr_halogen'
            ]).union(self.required_descriptors)),

            'Density': list(set([
                'AvgIpc','BalabanJ','BertzCT','Chi0','Chi0n','Chi0v','Chi1','Chi1n','Chi1v',
                'Chi2n','Chi2v','Chi3n','Chi3v','Chi4n','EState_VSA10','EState_VSA5',
                'EState_VSA7','EState_VSA8','EState_VSA9','ExactMolWt','FpDensityMorgan1',
                'FpDensityMorgan2','FpDensityMorgan3','FractionCSP3','HallKierAlpha',
                'HeavyAtomMolWt','Kappa1','Kappa2','Kappa3','MaxAbsEStateIndex',
                'MaxEStateIndex','MinEStateIndex','MolLogP','MolMR','MolWt'
            ]).union(self.required_descriptors)),

            'Rg': list(set([
                'BalabanJ','BertzCT','Chi1','Chi3n','Chi4n','EState_VSA4','EState_VSA8',
                'FpDensityMorgan3','HallKierAlpha','Kappa3','MaxAbsEStateIndex','MolLogP',
                'NumHeteroatoms','NumHeterocycles','NumRotatableBonds','RingCount',
                'fr_halogen'
            ]).union(self.required_descriptors))
        }
    
    def get_canonical_smiles(self, smiles):
        """è·å–è§„èŒƒåŒ–SMILES"""
        try:
            mol = Chem.MolFromSmiles(smiles)
            if mol is None:
                return None
            return Chem.MolToSmiles(mol, canonical=True)
        except:
            return None
    
    def clean_and_validate_smiles(self, smiles):
        """æ¸…æ´—å’ŒéªŒè¯SMILESï¼ˆå‚è€ƒé“œç‰Œæ–¹æ¡ˆï¼Œä¸¥æ ¼è¿‡æ»¤ï¼‰"""
        if not isinstance(smiles, str) or len(smiles) == 0:
            return None
        
        # ç§»é™¤ç©ºç™½å­—ç¬¦
        smiles = smiles.strip()
        
        # å¦‚æœSMILESä¸ºç©ºï¼Œè¿”å›None
        if not smiles:
            return None
        
        # æ£€æŸ¥é—®é¢˜æ¨¡å¼ï¼ˆå‚è€ƒé“œç‰Œæ–¹æ¡ˆï¼‰
        bad_patterns = [
            '[R]', '[R1]', '[R2]', '[R3]', '[R4]', '[R5]', 
            "[R']", '[R"]', 'R1', 'R2', 'R3', 'R4', 'R5',
            '([R])', '([R1])', '([R2])'
        ]
        
        # æ£€æŸ¥ä»»ä½•åæ¨¡å¼
        for pattern in bad_patterns:
            if pattern in smiles:
                return None
        
        # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœåŒ…å«][ä¸”æœ‰Rç›¸å…³æ¨¡å¼ï¼Œå¯èƒ½æ˜¯èšåˆç‰©è®°å·
        if '][' in smiles and any(x in smiles for x in ['[R', 'R]']):
            return None
        
        # å°è¯•ç”¨RDKitè§£æ
        try:
            mol = Chem.MolFromSmiles(smiles)
            if mol is not None and mol.GetNumAtoms() > 0:
                return Chem.MolToSmiles(mol, canonical=True)
            else:
                # å¦‚æœRDKitè§£æå¤±è´¥ï¼Œä½†SMILESåŒ…å«[*]ï¼ˆèšåˆç‰©æ ‡è®°ï¼‰ï¼Œä»ç„¶ä¿ç•™
                if '[*]' in smiles and len(smiles) > 10:
                    # åŸºæœ¬çš„èšåˆç‰©SMILESéªŒè¯
                    if smiles.count('(') == smiles.count(')') and smiles.count('[') == smiles.count(']'):
                        return smiles  # è¿”å›åŸå§‹SMILES
                return None
        except:
            # å¦‚æœRDKitè§£æå¤±è´¥ï¼Œä½†SMILESåŒ…å«[*]ï¼ˆèšåˆç‰©æ ‡è®°ï¼‰ï¼Œä»ç„¶ä¿ç•™
            if '[*]' in smiles and len(smiles) > 10:
                # åŸºæœ¬çš„èšåˆç‰©SMILESéªŒè¯
                if smiles.count('(') == smiles.count(')') and smiles.count('[') == smiles.count(']'):
                    return smiles  # è¿”å›åŸå§‹SMILES
            return None
    
    def augment_smiles_dataset(self, smiles_list, labels, num_augments=1):
        """SMILESæ•°æ®å¢å¼ºï¼ˆå‚è€ƒé“œç‰Œæ–¹æ¡ˆï¼Œå¢åŠ å¢å¼ºå€æ•°ï¼‰"""
        augmented_smiles = []
        augmented_labels = []
        
        for smiles, label in zip(smiles_list, labels):
            try:
                mol = Chem.MolFromSmiles(smiles)
                if mol is None:
                    continue
                    
                # æ·»åŠ åŸå§‹æ•°æ®
                augmented_smiles.append(smiles)
                augmented_labels.append(label)
                
                # ç”Ÿæˆå¢å¼ºæ•°æ®
                for _ in range(num_augments):
                    # éšæœºåŒ–SMILESè¡¨ç¤º
                    random_smiles = Chem.MolToSmiles(mol, doRandom=True)
                    augmented_smiles.append(random_smiles)
                    augmented_labels.append(label)
            except:
                continue
        
        return augmented_smiles, np.array(augmented_labels)
    
    def extract_tg_specific_features(self, mol, smiles):
        """æå–Tgä»»åŠ¡ç‰¹å¼‚æ€§ç‰¹å¾ï¼ˆåŸºäºèšçƒ¯çƒƒTgçŸ¥è¯†ï¼‰"""
        tg_features = {}
        
        try:
            # 1. èšçƒ¯çƒƒè¯†åˆ«ç‰¹å¾
            # èšä¸™çƒ¯(PP)æ¨¡å¼è¯†åˆ«
            pp_pattern = Chem.MolFromSmarts('[CH3][CH]([CH3])[CH2]')  # ä¸™çƒ¯å•å…ƒ
            pp_matches = len(mol.GetSubstructMatches(pp_pattern)) if pp_pattern else 0
            tg_features['pp_units'] = pp_matches
            
            # èšä¹™çƒ¯(PE)æ¨¡å¼è¯†åˆ«
            pe_pattern = Chem.MolFromSmarts('[CH2][CH2]')  # ä¹™çƒ¯å•å…ƒ
            pe_matches = len(mol.GetSubstructMatches(pe_pattern)) if pe_pattern else 0
            tg_features['pe_units'] = pe_matches
            
            # 2. åˆ†å­é‡ç›¸å…³ç‰¹å¾ï¼ˆå½±å“Tgçš„å…³é”®å› ç´ ï¼‰
            mol_wt = Descriptors.MolWt(mol)
            tg_features['mol_wt'] = mol_wt
            tg_features['log_mol_wt'] = np.log(mol_wt + 1)
            tg_features['mol_wt_normalized'] = mol_wt / mol.GetNumAtoms()  # å¹³å‡åŸå­è´¨é‡
            
            # 3. é“¾æŸ”æ€§ç‰¹å¾ï¼ˆå½±å“Tgçš„å…³é”®å› ç´ ï¼‰
            # å¯æ—‹è½¬é”®æ•°é‡ï¼ˆé“¾æŸ”æ€§æŒ‡æ ‡ï¼‰
            rotatable_bonds = Descriptors.NumRotatableBonds(mol)
            tg_features['rotatable_bonds'] = rotatable_bonds
            tg_features['flexibility_ratio'] = rotatable_bonds / mol.GetNumBonds() if mol.GetNumBonds() > 0 else 0
            
            # 4. ç»“æ™¶åº¦ç›¸å…³ç‰¹å¾
            # èŠ³é¦™ç¯æ•°é‡ï¼ˆå¢åŠ ç»“æ™¶åº¦ï¼Œæé«˜Tgï¼‰
            aromatic_rings = Descriptors.NumAromaticRings(mol)
            tg_features['aromatic_rings'] = aromatic_rings
            
            # è„‚è‚ªç¯æ•°é‡
            aliphatic_rings = Descriptors.NumAliphaticRings(mol)
            tg_features['aliphatic_rings'] = aliphatic_rings
            
            # 5. ä¾§åŸºç‰¹å¾ï¼ˆå½±å“Tgçš„å…³é”®å› ç´ ï¼‰
            # ç”²åŸºä¾§åŸºæ•°é‡
            methyl_pattern = Chem.MolFromSmarts('[CH3]')
            methyl_count = len(mol.GetSubstructMatches(methyl_pattern)) if methyl_pattern else 0
            tg_features['methyl_groups'] = methyl_count
            
            # å¤§ä½“ç§¯ä¾§åŸºè¯†åˆ«
            bulky_pattern = Chem.MolFromSmarts('[C]([C])([C])([C])')  # å­£ç¢³
            bulky_count = len(mol.GetSubstructMatches(bulky_pattern)) if bulky_pattern else 0
            tg_features['bulky_groups'] = bulky_count
            
            # 6. æ°¢é”®ç‰¹å¾ï¼ˆå½±å“åˆ†å­é—´ä½œç”¨åŠ›ï¼‰
            h_donors = Descriptors.NumHDonors(mol)
            h_acceptors = Descriptors.NumHAcceptors(mol)
            tg_features['h_bond_donors'] = h_donors
            tg_features['h_bond_acceptors'] = h_acceptors
            tg_features['h_bond_total'] = h_donors + h_acceptors
            
            # 7. èšçƒ¯çƒƒç‰¹å¼‚æ€§Tgé¢„æµ‹ç‰¹å¾
            # åŸºäºPPå’ŒPEçš„TgèŒƒå›´çŸ¥è¯†
            if pp_matches > 0:
                # PPå…¸å‹Tgçº¦-20Â°C
                tg_features['polyolefin_type'] = 1  # PPç±»å‹
                tg_features['expected_tg_range'] = -20
            elif pe_matches > 0:
                # PEå…¸å‹Tgçº¦-100Â°Cåˆ°-80Â°C
                tg_features['polyolefin_type'] = 2  # PEç±»å‹
                tg_features['expected_tg_range'] = -90
            else:
                tg_features['polyolefin_type'] = 0  # å…¶ä»–ç±»å‹
                tg_features['expected_tg_range'] = 0
            
            # 8. é“¾åˆšæ€§æŒ‡æ ‡
            # åŒé”®æ•°é‡ï¼ˆå¢åŠ é“¾åˆšæ€§ï¼‰
            double_bonds = len([bond for bond in mol.GetBonds() if bond.GetBondType() == Chem.BondType.DOUBLE])
            tg_features['double_bonds'] = double_bonds
            
            # ä¸‰é”®æ•°é‡ï¼ˆæ˜¾è‘—å¢åŠ é“¾åˆšæ€§ï¼‰
            triple_bonds = len([bond for bond in mol.GetBonds() if bond.GetBondType() == Chem.BondType.TRIPLE])
            tg_features['triple_bonds'] = triple_bonds
            
            # 9. åˆ†å­å½¢çŠ¶ç‰¹å¾
            tg_features['asphericity'] = rdMolDescriptors.Asphericity(mol)
            tg_features['eccentricity'] = rdMolDescriptors.Eccentricity(mol)
            tg_features['inertial_shape_factor'] = rdMolDescriptors.InertialShapeFactor(mol)
            
        except Exception as e:
            # å¦‚æœç‰¹å¾æå–å¤±è´¥ï¼Œå¡«å……é»˜è®¤å€¼
            for key in ['pp_units', 'pe_units', 'mol_wt', 'log_mol_wt', 'mol_wt_normalized',
                       'rotatable_bonds', 'flexibility_ratio', 'aromatic_rings', 'aliphatic_rings',
                       'methyl_groups', 'bulky_groups', 'h_bond_donors', 'h_bond_acceptors', 'h_bond_total',
                       'polyolefin_type', 'expected_tg_range', 'double_bonds', 'triple_bonds',
                       'asphericity', 'eccentricity', 'inertial_shape_factor']:
                tg_features[key] = 0
        
        return tg_features

    def extract_polymer_class_features(self, mol, smiles):
        """æå–åŸºäºèšåˆç‰©åˆ†ç±»çš„ç‰¹å¼‚æ€§ç‰¹å¾ï¼ˆåŸºäºPolyInfoæ•°æ®åº“åˆ†ç±»ï¼‰"""
        polymer_features = {}

        try:
            # 1. èšçƒ¯çƒƒç±»ç‰¹å¾ (Polyolefins)
            # æ£€æµ‹èšä¹™çƒ¯ã€èšä¸™çƒ¯ç­‰ç»“æ„
            polyolefin_score = 0

            # ä¹™çƒ¯å•å…ƒ [CH2-CH2]
            ethylene_pattern = Chem.MolFromSmarts('[CH2][CH2]')
            if ethylene_pattern:
                ethylene_matches = len(mol.GetSubstructMatches(ethylene_pattern))
                polyolefin_score += ethylene_matches * 2

            # ä¸™çƒ¯å•å…ƒ [CH2-CH(CH3)]
            propylene_pattern = Chem.MolFromSmarts('[CH2][CH]([CH3])')
            if propylene_pattern:
                propylene_matches = len(mol.GetSubstructMatches(propylene_pattern))
                polyolefin_score += propylene_matches * 3

            polymer_features['polyolefin_score'] = polyolefin_score

            # 2. èšé…¯ç±»ç‰¹å¾ (Polyesters)
            # é…¯é”® [-COO-]
            ester_pattern = Chem.MolFromSmarts('[C](=O)[O][C]')
            ester_matches = len(mol.GetSubstructMatches(ester_pattern)) if ester_pattern else 0
            polymer_features['ester_bonds'] = ester_matches

            # èŠ³é¦™æ—é…¯ï¼ˆå¦‚PETï¼‰
            aromatic_ester_pattern = Chem.MolFromSmarts('c[C](=O)[O]')
            aromatic_ester_matches = len(mol.GetSubstructMatches(aromatic_ester_pattern)) if aromatic_ester_pattern else 0
            polymer_features['aromatic_ester_bonds'] = aromatic_ester_matches

            # 3. èšé…°èƒºç±»ç‰¹å¾ (Polyamides)
            # é…°èƒºé”® [-CONH-]
            amide_pattern = Chem.MolFromSmarts('[C](=O)[NH]')
            amide_matches = len(mol.GetSubstructMatches(amide_pattern)) if amide_pattern else 0
            polymer_features['amide_bonds'] = amide_matches

            # èŠ³é¦™æ—é…°èƒºï¼ˆå¦‚èŠ³çº¶ï¼‰
            aromatic_amide_pattern = Chem.MolFromSmarts('c[C](=O)[NH]')
            aromatic_amide_matches = len(mol.GetSubstructMatches(aromatic_amide_pattern)) if aromatic_amide_pattern else 0
            polymer_features['aromatic_amide_bonds'] = aromatic_amide_matches

            # 4. èšé…°äºšèƒºç±»ç‰¹å¾ (Polyimides)
            # é…°äºšèƒºç¯ç»“æ„
            imide_pattern = Chem.MolFromSmarts('[C](=O)[N]([C](=O))')
            imide_matches = len(mol.GetSubstructMatches(imide_pattern)) if imide_pattern else 0
            polymer_features['imide_groups'] = imide_matches

            # èŠ³é¦™æ—é…°äºšèƒºï¼ˆé«˜æ€§èƒ½èšé…°äºšèƒºï¼‰
            aromatic_imide_pattern = Chem.MolFromSmarts('c1ccc2c(c1)[C](=O)[N]([C](=O)2)')
            aromatic_imide_matches = len(mol.GetSubstructMatches(aromatic_imide_pattern)) if aromatic_imide_pattern else 0
            polymer_features['aromatic_imide_groups'] = aromatic_imide_matches

            # 5. èšé†šç±»ç‰¹å¾ (Polyethers)
            # é†šé”® [-O-]
            ether_pattern = Chem.MolFromSmarts('[C][O][C]')
            ether_matches = len(mol.GetSubstructMatches(ether_pattern)) if ether_pattern else 0
            polymer_features['ether_bonds'] = ether_matches

            # èŠ³é¦™æ—é†šï¼ˆå¦‚PEEKï¼‰
            aromatic_ether_pattern = Chem.MolFromSmarts('c[O]c')
            aromatic_ether_matches = len(mol.GetSubstructMatches(aromatic_ether_pattern)) if aromatic_ether_pattern else 0
            polymer_features['aromatic_ether_bonds'] = aromatic_ether_matches

            # 6. èšä¸™çƒ¯é…¸ç±»ç‰¹å¾ (Polyacrylics)
            # ä¸™çƒ¯é…¸é…¯ç»“æ„ [CH2=CH-COO-]
            acrylate_pattern = Chem.MolFromSmarts('[CH2]=[CH][C](=O)[O]')
            acrylate_matches = len(mol.GetSubstructMatches(acrylate_pattern)) if acrylate_pattern else 0
            polymer_features['acrylate_groups'] = acrylate_matches

            # ç”²åŸºä¸™çƒ¯é…¸é…¯ç»“æ„
            methacrylate_pattern = Chem.MolFromSmarts('[CH2]=[C]([CH3])[C](=O)[O]')
            methacrylate_matches = len(mol.GetSubstructMatches(methacrylate_pattern)) if methacrylate_pattern else 0
            polymer_features['methacrylate_groups'] = methacrylate_matches

            # 7. èšä¹™çƒ¯åŸºç±»ç‰¹å¾ (Polyvinyls)
            # ä¹™çƒ¯åŸºç»“æ„ [CH2=CH-]
            vinyl_pattern = Chem.MolFromSmarts('[CH2]=[CH]')
            vinyl_matches = len(mol.GetSubstructMatches(vinyl_pattern)) if vinyl_pattern else 0
            polymer_features['vinyl_groups'] = vinyl_matches

            # è‹¯ä¹™çƒ¯å•å…ƒï¼ˆèšè‹¯ä¹™çƒ¯ï¼‰
            styrene_pattern = Chem.MolFromSmarts('[CH2]=[CH]c1ccccc1')
            styrene_matches = len(mol.GetSubstructMatches(styrene_pattern)) if styrene_pattern else 0
            polymer_features['styrene_groups'] = styrene_matches

            # 8. ç»¼åˆåˆ†ç±»è¯„åˆ†
            # åŸºäºç»“æ„ç‰¹å¾é¢„æµ‹èšåˆç‰©ç±»åˆ«å€¾å‘
            total_bonds = mol.GetNumBonds()
            if total_bonds > 0:
                polymer_features['ester_ratio'] = ester_matches / total_bonds
                polymer_features['amide_ratio'] = amide_matches / total_bonds
                polymer_features['ether_ratio'] = ether_matches / total_bonds
                polymer_features['aromatic_ratio'] = Descriptors.NumAromaticRings(mol) / total_bonds
            else:
                polymer_features['ester_ratio'] = 0
                polymer_features['amide_ratio'] = 0
                polymer_features['ether_ratio'] = 0
                polymer_features['aromatic_ratio'] = 0

            # 9. é«˜æ€§èƒ½èšåˆç‰©ç‰¹å¾
            # åˆšæ€§é“¾æ®µç‰¹å¾ï¼ˆå½±å“é«˜Tgï¼‰
            rigid_aromatic_pattern = Chem.MolFromSmarts('c1ccc2ccccc2c1')  # è˜ç¯ç­‰åˆšæ€§ç»“æ„
            rigid_matches = len(mol.GetSubstructMatches(rigid_aromatic_pattern)) if rigid_aromatic_pattern else 0
            polymer_features['rigid_segments'] = rigid_matches

            # æ‚ç¯ç‰¹å¾ï¼ˆå¦‚èšè‹¯å¹¶å’ªå”‘ç­‰ï¼‰
            heterocycle_count = Descriptors.NumHeterocycles(mol)
            polymer_features['heterocycle_density'] = heterocycle_count / mol.GetNumAtoms() if mol.GetNumAtoms() > 0 else 0

        except Exception as e:
            # å¦‚æœç‰¹å¾æå–å¤±è´¥ï¼Œå¡«å……é»˜è®¤å€¼
            default_features = [
                'polyolefin_score', 'ester_bonds', 'aromatic_ester_bonds', 'amide_bonds', 'aromatic_amide_bonds',
                'imide_groups', 'aromatic_imide_groups', 'ether_bonds', 'aromatic_ether_bonds',
                'acrylate_groups', 'methacrylate_groups', 'vinyl_groups', 'styrene_groups',
                'ester_ratio', 'amide_ratio', 'ether_ratio', 'aromatic_ratio',
                'rigid_segments', 'heterocycle_density'
            ]
            for feature in default_features:
                polymer_features[feature] = 0

        return polymer_features

    def smiles_to_combined_features(self, smiles_list, task_filter, radius=2, n_bits=128):
        """å°†SMILESè½¬æ¢ä¸ºç»„åˆç‰¹å¾ï¼ˆå®Œå…¨é›†æˆGitHubé¡¹ç›®çš„ä¸“ä¸šç‰¹å¾å·¥ç¨‹ï¼‰"""

        print(f"ğŸš€ å¼€å§‹ç‰¹å¾æå–: {len(smiles_list)} ä¸ªSMILES")

        # ä¼˜å…ˆä½¿ç”¨GitHubé£æ ¼ä¸“ä¸šç‰¹å¾å·¥ç¨‹
        if self.use_github_features:
            print("   ğŸ¯ ä½¿ç”¨GitHubé£æ ¼ä¸“ä¸šç‰¹å¾å·¥ç¨‹æ–¹æ³•")
            fingerprints, descriptors, valid_smiles, invalid_indices = self._extract_github_style_features(smiles_list, task_filter)

            # å¦‚æœGitHubé£æ ¼æ–¹æ³•æˆåŠŸï¼Œç›´æ¥è¿”å›
            if fingerprints.size > 0:
                print(f"   âœ… GitHubé£æ ¼ç‰¹å¾å·¥ç¨‹æˆåŠŸ: {fingerprints.shape[1]} æŒ‡çº¹ç‰¹å¾")
                return fingerprints, descriptors, valid_smiles, invalid_indices
            else:
                print("   âš ï¸ GitHubé£æ ¼ç‰¹å¾å·¥ç¨‹è¿”å›ç©ºç»“æœï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
        else:
            print("   âš ï¸ GitHubé£æ ¼ç‰¹å¾å·¥ç¨‹æœªå¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿç‰¹å¾å·¥ç¨‹")

        # å¦åˆ™ä½¿ç”¨ä¼ ç»Ÿç‰¹å¾å·¥ç¨‹
        fingerprints = []
        descriptors = []
        valid_smiles = []
        invalid_indices = []

        # è·å–å¤šç§æŒ‡çº¹ç”Ÿæˆå™¨
        morgan_gen = GetMorganGenerator(radius=radius, fpSize=n_bits)
        atom_pair_gen = GetAtomPairGenerator(fpSize=n_bits)
        torsion_gen = GetTopologicalTorsionGenerator(fpSize=n_bits)
        
        # æè¿°ç¬¦å‡½æ•°å­—å…¸
        descriptor_functions = {name: func for name, func in Descriptors.descList if name in task_filter}
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºTgä»»åŠ¡
        is_tg_task = 'Tg' in str(task_filter) or any('Tg' in str(f) for f in task_filter)
        
        for i, smiles in enumerate(smiles_list):
            try:
                mol = Chem.MolFromSmiles(smiles)
                if mol is not None:
                    # 1. å¤šç§æŒ‡çº¹ç»„åˆ
                    morgan_fp = morgan_gen.GetFingerprint(mol)
                    atom_pair_fp = atom_pair_gen.GetFingerprint(mol)
                    torsion_fp = torsion_gen.GetFingerprint(mol)
                    maccs_fp = MACCSkeys.GenMACCSKeys(mol)
                    
                    # ç»„åˆæ‰€æœ‰æŒ‡çº¹
                    combined_fp = np.concatenate([
                        np.array(morgan_fp),
                        np.array(atom_pair_fp),
                        np.array(torsion_fp),
                        np.array(maccs_fp)
                    ])
                    
                    # 2. åˆ†å­æè¿°ç¬¦
                    descriptor_values = {}
                    
                    # RDKitæè¿°ç¬¦
                    for name, func in descriptor_functions.items():
                        try:
                            descriptor_values[name] = func(mol)
                        except:
                            descriptor_values[name] = None
                    
                    # åŸºç¡€æè¿°ç¬¦
                    descriptor_values['MolWt'] = Descriptors.MolWt(mol)
                    descriptor_values['LogP'] = Descriptors.MolLogP(mol)
                    descriptor_values['TPSA'] = Descriptors.TPSA(mol)
                    descriptor_values['RotatableBonds'] = Descriptors.NumRotatableBonds(mol)
                    descriptor_values['NumAtoms'] = mol.GetNumAtoms()
                    descriptor_values['SMILES'] = smiles
                    
                    # 3. Tgç‰¹å¼‚æ€§ç‰¹å¾ï¼ˆä»…å¯¹Tgä»»åŠ¡æ·»åŠ ï¼‰
                    if is_tg_task:
                        tg_features = self.extract_tg_specific_features(mol, smiles)
                        descriptor_values.update(tg_features)

                        # 4. èšåˆç‰©åˆ†ç±»ç‰¹å¼‚æ€§ç‰¹å¾ï¼ˆåŸºäºPolyInfoæ•°æ®ï¼‰
                        polymer_class_features = self.extract_polymer_class_features(mol, smiles)
                        descriptor_values.update(polymer_class_features)
                    
                    # 4. å›¾ç»“æ„ç‰¹å¾
                    try:
                        adj = rdmolops.GetAdjacencyMatrix(mol)
                        G = nx.from_numpy_array(adj)
                        
                        if nx.is_connected(G):
                            descriptor_values['graph_diameter'] = nx.diameter(G)
                            descriptor_values['avg_shortest_path'] = nx.average_shortest_path_length(G)
                        else:
                            descriptor_values['graph_diameter'] = 0
                            descriptor_values['avg_shortest_path'] = 0
                        
                        descriptor_values['num_cycles'] = len(list(nx.cycle_basis(G)))
                    except:
                        descriptor_values['graph_diameter'] = None
                        descriptor_values['avg_shortest_path'] = None
                        descriptor_values['num_cycles'] = None
                    
                    fingerprints.append(combined_fp)
                    descriptors.append(descriptor_values)
                    valid_smiles.append(smiles)
                else:
                    # æ— æ•ˆSMILESï¼Œè·³è¿‡è€Œä¸æ˜¯å¡«å……é›¶å‘é‡
                    invalid_indices.append(i)
                    continue
            except Exception as e:
                # ç‰¹å¾æå–å¤±è´¥ï¼Œè·³è¿‡è€Œä¸æ˜¯å¡«å……é›¶å‘é‡
                print(f"      âš ï¸ SMILESç‰¹å¾æå–å¤±è´¥: {smiles[:50]}... é”™è¯¯: {str(e)[:50]}")
                invalid_indices.append(i)
                continue
        
        # ç¡®ä¿è¿”å›çš„æ•°ç»„é•¿åº¦ä¸€è‡´
        if len(fingerprints) != len(descriptors) or len(fingerprints) != len(valid_smiles):
            print(f"      âš ï¸ ç‰¹å¾æå–é•¿åº¦ä¸ä¸€è‡´: fp={len(fingerprints)}, desc={len(descriptors)}, smiles={len(valid_smiles)}")

        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„ç‰¹å¾ï¼Œè¿”å›ç©ºæ•°ç»„
        if len(fingerprints) == 0:
            print(f"      âŒ æ²¡æœ‰æœ‰æ•ˆçš„ç‰¹å¾æå–ç»“æœï¼Œè¿”å›ç©ºæ•°ç»„")
            return np.array([]), [], [], list(range(len(smiles_list)))

        print(f"      âœ… ç‰¹å¾æå–å®Œæˆ: {len(fingerprints)}/{len(smiles_list)} æœ‰æ•ˆæ ·æœ¬")
        return np.array(fingerprints), descriptors, valid_smiles, invalid_indices
    
    def augment_dataset_with_gmm(self, X, y, n_samples=1000, n_components=5, random_state=42):
        """ä½¿ç”¨é«˜æ–¯æ··åˆæ¨¡å‹è¿›è¡Œæ•°æ®å¢å¼º"""
        # ç§»é™¤ç¼ºå¤±å€¼
        mask = ~np.isnan(y)
        X_clean = X[mask]
        y_clean = y[mask]
        
        if len(X_clean) < n_components:
            # æ•°æ®å¤ªå°‘ï¼Œç›´æ¥è¿”å›åŸæ•°æ®
            return pd.DataFrame(np.column_stack([X_clean, y_clean]), columns=[f'Feature_{i}' for i in range(X_clean.shape[1])] + ['Target'])
        
        # æ‹Ÿåˆé«˜æ–¯æ··åˆæ¨¡å‹
        gmm = GaussianMixture(n_components=n_components, random_state=random_state)
        
        # ç»„åˆç‰¹å¾å’Œæ ‡ç­¾
        combined_data = np.column_stack([X_clean, y_clean])
        gmm.fit(combined_data)
        
        # ç”Ÿæˆæ–°æ ·æœ¬
        synthetic_data, _ = gmm.sample(n_samples)
        
        # ç»„åˆåŸå§‹æ•°æ®å’Œåˆæˆæ•°æ®
        augmented_data = np.vstack([combined_data, synthetic_data])
        
        # åˆ›å»ºDataFrame
        columns = [f'Feature_{i}' for i in range(X_clean.shape[1])] + ['Target']
        return pd.DataFrame(augmented_data, columns=columns)
    
    def load_and_split_data(self):
        """åŠ è½½æ•°æ®å¹¶è¿›è¡Œä¸‰å±‚åˆ†å‰²ï¼ˆå‚è€ƒé“œç‰Œæ–¹æ¡ˆï¼‰"""
        print("ğŸ“Š åŠ è½½å’Œå¤„ç†æ•°æ®...")
        
        # åŠ è½½æ•°æ®
        train_df = pd.read_csv('train.csv')
        test_df = pd.read_csv('test.csv')
        
        print(f"åŸå§‹æ•°æ®: è®­ç»ƒé›†{train_df.shape}, æµ‹è¯•é›†{test_df.shape}")
        
        # æ¸…æ´—SMILES
        print("ğŸ”„ æ¸…æ´—å’ŒéªŒè¯SMILES...")
        train_df['SMILES'] = train_df['SMILES'].apply(self.clean_and_validate_smiles)
        test_df['SMILES'] = test_df['SMILES'].apply(self.clean_and_validate_smiles)
        
        # ç§»é™¤æ— æ•ˆSMILES
        invalid_train = train_df['SMILES'].isnull().sum()
        invalid_test = test_df['SMILES'].isnull().sum()
        
        print(f"   ç§»é™¤äº†{invalid_train}ä¸ªæ— æ•ˆè®­ç»ƒSMILES")
        print(f"   ç§»é™¤äº†{invalid_test}ä¸ªæ— æ•ˆæµ‹è¯•SMILES")
        
        train_df = train_df[train_df['SMILES'].notnull()].reset_index(drop=True)
        test_df = test_df[test_df['SMILES'].notnull()].reset_index(drop=True)
        
        # é›†æˆå¤–éƒ¨æ•°æ®ï¼ˆå‚è€ƒé“œç‰Œæ–¹æ¡ˆï¼‰
        train_df = self.integrate_external_data(train_df)
        
        # ä¸‰å±‚æ•°æ®åˆ†å‰²ï¼ˆå‚è€ƒé“œç‰Œæ–¹æ¡ˆï¼‰
        temp_df, self.dev_test = train_test_split(
            train_df, test_size=0.2, random_state=42, shuffle=True
        )
        self.dev_train, self.dev_val = train_test_split(
            temp_df, test_size=0.25, random_state=42, shuffle=True
        )
        
        print(f"æ•°æ®åˆ†å‰²ç»“æœ:")
        print(f"   Dev train: {len(self.dev_train)} ({len(self.dev_train)/len(train_df):.2%})")
        print(f"   Dev valid: {len(self.dev_val)} ({len(self.dev_val)/len(train_df):.2%})")
        print(f"   Dev test:  {len(self.dev_test)} ({len(self.dev_test)/len(train_df):.2%})")
        
        # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ†ç¦»å­è¡¨
        self.subtables = self.separate_subtables(train_df)
        self.test_df = test_df
        
        # æ‰“å°æ¯ä¸ªä»»åŠ¡çš„æ ·æœ¬æ•°é‡
        for task in self.task_names:
            count = len(self.subtables[task])
            print(f"   {task}: {count} ä¸ªæ ·æœ¬")
        
        return train_df
    
    def safe_load_dataset(self, path, target, processor_func, description):
        """å®‰å…¨åŠ è½½æ•°æ®é›†ï¼ˆæ”¯æŒåªæœ‰SMILESçš„æ•°æ®é›†ï¼‰"""
        try:
            # å¯¹äºç‰¹æ®Šæ•°æ®é›†ï¼Œè®©processorç›´æ¥å¤„ç†æ–‡ä»¶è·¯å¾„
            if 'Bicerano' in description or 'BIMOG' in description or 'Github' in description:
                data = processor_func(path)  # ä¼ å…¥æ–‡ä»¶è·¯å¾„ï¼Œè®©processorè‡ªå·±è¯»å–æ–‡ä»¶
            else:
                # å…¶ä»–æ•°æ®é›†æ­£å¸¸è¯»å–
                if path.endswith('.xlsx'):
                    data = pd.read_excel(path)
                else:
                    data = pd.read_csv(path)

                # åº”ç”¨å¤„ç†å‡½æ•°
                data = processor_func(data)
            
            # éªŒè¯å¿…è¦åˆ—
            if 'SMILES' not in data.columns:
                print(f"   âš ï¸ {description}: ç¼ºå°‘SMILESåˆ—")
                return None
            
            # å¦‚æœtargetä¸ºNoneï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯å¤šç›®æ ‡æ•°æ®é›†
            if target is None:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•ç›®æ ‡å˜é‡
                available_targets = [col for col in self.task_names if col in data.columns]
                if available_targets:
                    # è¿™æ˜¯å¤šç›®æ ‡æ•°æ®é›†
                    data = data.dropna(subset=['SMILES'])
                    if len(data) == 0:
                        print(f"   âš ï¸ {description}: æ¸…æ´—åæ•°æ®ä¸ºç©º")
                        return None
                    print(f"   âœ… {description}: åŠ è½½ {len(data)} æ ·æœ¬ï¼ŒåŒ…å«ç›®æ ‡: {available_targets}")
                    return ('MULTI_TARGET', data)
                else:
                    # è¿™æ˜¯åªæœ‰SMILESçš„æ•°æ®é›†
                    data = data.dropna(subset=['SMILES'])
                    if len(data) == 0:
                        print(f"   âš ï¸ {description}: æ¸…æ´—åæ•°æ®ä¸ºç©º")
                        return None
                    print(f"   âœ… {description}: åŠ è½½ {len(data)} SMILESæ ·æœ¬")
                    return ('SMILES_ONLY', data)
            
            # æœ‰ç›®æ ‡å˜é‡çš„æƒ…å†µ
            if target == 'Extended':
                # å¯¹äºExtendedæ•°æ®é›†ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•ç›®æ ‡å˜é‡
                available_targets = [col for col in self.task_names if col in data.columns]
                if not available_targets:
                    print(f"   âš ï¸ {description}: ç¼ºå°‘ç›®æ ‡å˜é‡åˆ—")
                    return None
            elif target not in data.columns:
                print(f"   âš ï¸ {description}: ç¼ºå°‘{target}åˆ—")
                return None
            
            # æ¸…æ´—æ•°æ®
            if target == 'Extended':
                # å¯¹äºExtendedæ•°æ®é›†ï¼Œåªæ¸…æ´—SMILESåˆ—ï¼Œä¿ç•™æ‰€æœ‰ç›®æ ‡å˜é‡
                data = data.dropna(subset=['SMILES'])
            else:
                data = data.dropna(subset=['SMILES', target])
            
            if len(data) > 0:
                print(f"   âœ… æˆåŠŸåŠ è½½ {description}: {len(data)} æ ·æœ¬")
                return (target, data)
            else:
                print(f"   âš ï¸ {description} æ•°æ®ä¸ºç©º")
                return None
        except Exception as e:
            print(f"   âŒ åŠ è½½ {description} å¤±è´¥: {str(e)[:50]}")
            return None
    
    def integrate_external_data(self, train_df):
        """é›†æˆå¤–éƒ¨æ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨æœ¬åœ°train_supplementæ•°æ®é›†ï¼Œå…¼å®¹Kaggleå¤–éƒ¨æ•°æ®é›†ï¼‰"""
        print("ğŸ”„ é›†æˆå¤–éƒ¨æ•°æ®...")
        
        # åˆ›å»ºæ‰©å±•è®­ç»ƒé›†
        train_extended = train_df[['SMILES'] + self.task_names].copy()
        
        # å¤–éƒ¨æ•°æ®é›†åˆ—è¡¨
        external_datasets = []
        
        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°train_supplementæ•°æ®é›†
        local_dataset_configs = [
            {
                'path': 'train_supplement/dataset1.csv',
                'target': 'Tc',
                'processor': lambda df: df.rename(columns={'TC_mean': 'Tc'}),
                'description': 'Local Tc data (dataset1)'
            },
            {
                'path': 'train_supplement/dataset2.csv',
                'target': None,  # åªæœ‰SMILESï¼Œæ²¡æœ‰ç›®æ ‡å˜é‡
                'processor': lambda df: df[['SMILES']] if 'SMILES' in df.columns else df,
                'description': 'Local SMILES data (dataset2)'
            },
            {
                'path': 'train_supplement/dataset3.csv',
                'target': 'Tg',
                'processor': lambda df: df[['SMILES', 'Tg']] if 'Tg' in df.columns else df,
                'description': 'Local Tg data (dataset3)'
            },
            {
                'path': 'train_supplement/dataset4.csv',
                'target': 'FFV',
                'processor': lambda df: df[['SMILES', 'FFV']] if 'FFV' in df.columns else df,
                'description': 'Local FFV data (dataset4)'
            },
            # âŒ ç§»é™¤é‡å çš„newdataæ•°æ®é›†ï¼ˆå·²åœ¨newdata5ä¸­åŒ…å«ï¼‰
            # ä»¥ä¸‹æ•°æ®é›†ä¸newdata5å®Œå…¨é‡å ï¼Œå·²ç§»é™¤é¿å…é‡å¤ï¼š
            # - newdata/Tc_SMILES.csv (874 samples) â†’ newdata5/external Polymer/Tc_SMILES.csv
            # - newdata/TgSS_enriched_cleaned.csv (7284 samples) â†’ newdata5/Extra dataset/TgSS_enriched_cleaned.csv
            # - newdata/archive/JCIM_sup_bigsmiles.csv (662 samples) â†’ newdata5/external Polymer/JCIM_sup_bigsmiles.csv
            # - newdata/archive/data_tg3.xlsx (501 samples) â†’ newdata5/external Polymer/data_tg3.xlsx
            # - newdata/archive/data_dnst1.xlsx (787 samples) â†’ newdata5/external Polymer/data_dnst1.xlsx
            # æ–°å¢ç”¨æˆ·ä¸Šä¼ çš„é«˜è´¨é‡æ•°æ®é›†
            {
                'path': 'newdata/Bicerano_bigsmiles.csv',
                'target': 'Tg',
                'processor': lambda df: self._process_bicerano_data(df),
                'description': 'Bicerano BigSMILES Tg data (304 samples)'
            },
            {
                'path': 'newdata/extended_polymer_dataset.csv',
                'target': 'Extended',  # åŒ…å«å¤šä¸ªç›®æ ‡å˜é‡
                'processor': lambda df: self._process_extended_polymer_data(df),
                'description': 'Extended polymer dataset (1088 samples)'
            },
            {
                'path': 'newdata/BIMOG_database_v1.0_data.csv',
                'target': 'Tg',
                'processor': lambda df: self._process_bimog_data(df),
                'description': 'BIMOG database Tg data (635 samples)'
            },
            # ğŸ¯ NewData5é«˜ä»·å€¼æ•°æ®é›† - æ›¿ä»£é‡å æ•°æ®å¹¶æä¾›æ–°åŠŸèƒ½
            {
                'path': 'newdata5/external Polymer/PI1070.csv',
                'target': None,  # å¤šç›®æ ‡æ•°æ®é›†ï¼ŒåŒ…å«å¯†åº¦+Rg+çƒ­å¯¼ç‡ç­‰
                'processor': lambda df: self._process_pi1070_data(df),
                'description': 'ğŸ¥‡ PI1070 Multi-Property Dataset (1,077 samples, 157 features)'
            },
            {
                'path': 'newdata5/polymer_tg_density/tg_density.csv',
                'target': None,  # å¤šç›®æ ‡æ•°æ®é›†ï¼ŒåŒ…å«Tg+å¯†åº¦
                'processor': lambda df: self._process_tg_density_data(df),
                'description': 'ğŸ¥ˆ Experimental Tg+Density Dataset (194 samples)'
            },
            {
                'path': 'newdata5/POINT2 Dataset/Tg_SMILES_class_pid_polyinfo_median.csv',
                'target': 'Tg',
                'processor': lambda df: self._process_polyinfo_tg_data(df),
                'description': 'ğŸ¥‰ PolyInfo Authoritative Tg Database (7,208 samples with polymer classification)'
            },
            # ğŸ¯ Githubé¡¹ç›®é«˜ä»·å€¼ç»„ä»¶é›†æˆ
            {
                'path': 'Github/Polymer_Tg_-main/Data/32_Conjugate_Polymer.txt',
                'target': 'Tg',
                'processor': lambda df: self._process_github_conjugated_polymers(df),
                'description': 'ğŸ¥‡ Githubé«˜è´¨é‡å…±è½­èšåˆç‰©æµ‹è¯•é›† (32 samples, -30~215Â°C)'
            },
        ]
        
        # Kaggleå¤–éƒ¨æ•°æ®é›†ï¼ˆä½œä¸ºå¤‡é€‰ï¼‰
        kaggle_dataset_configs = [
            {
                'path': '/kaggle/input/tc-smiles/Tc_SMILES.csv',
                'target': 'Tc',
                'processor': lambda df: df.rename(columns={'TC_mean': 'Tc'}),
                'description': 'Kaggle Tc data'
            },
            {
                'path': '/kaggle/input/tg-smiles-pid-polymer-class/TgSS_enriched_cleaned.csv',
                'target': 'Tg',
                'processor': lambda df: df[['SMILES', 'Tg']] if 'Tg' in df.columns else df,
                'description': 'Kaggle TgSS enriched data'
            },
            {
                'path': '/kaggle/input/smiles-extra-data/JCIM_sup_bigsmiles.csv',
                'target': 'Tg',
                'processor': lambda df: df[['SMILES', 'Tg (C)']].rename(columns={'Tg (C)': 'Tg'}),
                'description': 'Kaggle JCIM Tg data'
            },
            {
                'path': '/kaggle/input/smiles-extra-data/data_tg3.xlsx',
                'target': 'Tg',
                'processor': lambda df: df.rename(columns={'Tg [K]': 'Tg'}).assign(Tg=lambda x: x['Tg'] - 273.15),
                'description': 'Kaggle Xlsx Tg data'
            },
            {
                'path': '/kaggle/input/smiles-extra-data/data_dnst1.xlsx',
                'target': 'Density',
                'processor': lambda df: df.rename(columns={'density(g/cm3)': 'Density'})[['SMILES', 'Density']]
                            .query('SMILES.notnull() and Density.notnull() and Density != "nylon"')
                            .assign(Density=lambda x: x['Density'].astype(float) - 0.118),
                'description': 'Kaggle Density data'
            },
            {
                'path': '/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset4.csv',
                'target': 'FFV',
                'processor': lambda df: df[['SMILES', 'FFV']] if 'FFV' in df.columns else df,
                'description': 'Kaggle dataset 4'
            }
        ]
        
        # é¦–å…ˆå°è¯•åŠ è½½æœ¬åœ°æ•°æ®é›†
        print("ğŸ“‚ å°è¯•åŠ è½½æœ¬åœ°train_supplementæ•°æ®é›†...")
        for config in local_dataset_configs:
            result = self.safe_load_dataset(
                config['path'], 
                config['target'], 
                config['processor'], 
                config['description']
            )
            if result is not None:
                external_datasets.append(result)
        
        # å¦‚æœæœ¬åœ°æ•°æ®é›†ä¸è¶³ï¼Œå°è¯•åŠ è½½Kaggleæ•°æ®é›†
        if len(external_datasets) < 3:
            print("ğŸ“‚ å°è¯•åŠ è½½Kaggleå¤–éƒ¨æ•°æ®é›†...")
            for config in kaggle_dataset_configs:
                result = self.safe_load_dataset(
                    config['path'], 
                    config['target'], 
                    config['processor'], 
                    config['description']
                )
                if result is not None:
                    external_datasets.append(result)
        
        # é›†æˆå¤–éƒ¨æ•°æ®
        print(f"\nğŸ“Š æœ€ç»ˆè®­ç»ƒæ•°æ®:")
        print(f"   åŸå§‹æ ·æœ¬: {len(train_df)}")
        
        for target, dataset in external_datasets:
            if target == 'SMILES_ONLY':
                print(f"   å¤„ç†çº¯SMILESæ•°æ®...")
                # å¯¹äºåªæœ‰SMILESçš„æ•°æ®ï¼Œæˆ‘ä»¬åªæ˜¯æ‰©å±•SMILESæ± ï¼Œä¸æ·»åŠ æ–°çš„ç›®æ ‡å€¼
                new_smiles = set(dataset['SMILES']) - set(train_extended['SMILES'])
                if new_smiles:
                    new_rows = []
                    for smiles in new_smiles:
                        new_row = {'SMILES': smiles}
                        for task in self.task_names:
                            new_row[task] = np.nan
                        new_rows.append(new_row)
                    new_df = pd.DataFrame(new_rows)
                    train_extended = pd.concat([train_extended, new_df], axis=0, ignore_index=True)
                    print(f"      æ·»åŠ äº† {len(new_smiles)} ä¸ªæ–°SMILES")
            elif target == 'MULTI_TARGET':
                print(f"   å¤„ç†å¤šç›®æ ‡æ•°æ®é›†...")
                # å¯¹äºå¤šç›®æ ‡æ•°æ®é›†ï¼Œç›´æ¥åˆå¹¶æ‰€æœ‰å¯ç”¨çš„ç›®æ ‡å˜é‡
                available_targets = [col for col in self.task_names if col in dataset.columns]
                print(f"      å¯ç”¨ç›®æ ‡: {available_targets}")

                # ä½¿ç”¨å¤–éƒ¨æ•°æ®æ¸…æ´—å‡½æ•°
                for task_target in available_targets:
                    train_extended = self.add_external_data_clean(train_extended, dataset, task_target)
            elif target == 'Extended':
                print(f"   å¤„ç†æ‰©å±•å¤šç›®æ ‡æ•°æ®é›†...")
                # å¤„ç†åŒ…å«å¤šä¸ªç›®æ ‡å˜é‡çš„æ•°æ®é›†
                available_targets = [col for col in self.task_names if col in dataset.columns]
                for single_target in available_targets:
                    target_data = dataset[['SMILES', single_target]].dropna()
                    if len(target_data) > 0:
                        print(f"      âœ… å¤„ç†Extendedæ•°æ®é›†{single_target}ç›®æ ‡: {len(target_data)}ä¸ªæ ·æœ¬")
                        train_extended = self.add_external_data_clean(train_extended, target_data, single_target)
            else:
                print(f"   å¤„ç† {target} æ•°æ®...")
                train_extended = self.add_external_data_clean(train_extended, dataset, target)
        
        print(f"   æ‰©å±•æ ·æœ¬: {len(train_extended)}")
        print(f"   å¢åŠ : +{len(train_extended) - len(train_df)} æ ·æœ¬")
        
        # æ‰“å°æ¯ä¸ªä»»åŠ¡çš„æ ·æœ¬æ•°é‡
        for target in self.task_names:
            count = train_extended[target].notna().sum()
            original_count = train_df[target].notna().sum() if target in train_df.columns else 0
            gain = count - original_count
            print(f"   {target}: {count:,} æ ·æœ¬ (+{gain})")
        
        return train_extended
    
    def _process_bicerano_data(self, df):
        """å¤„ç†Bicerano BigSMILESæ•°æ®é›†"""
        try:
            # ç›´æ¥è¯»å–æ–‡ä»¶ï¼Œä½¿ç”¨latin-1ç¼–ç ï¼ˆå·²ç¡®è®¤å¯ç”¨ï¼‰
            try:
                df = pd.read_csv('newdata/Bicerano_bigsmiles.csv', encoding='latin-1')
                print(f"      âœ… æˆåŠŸè¯»å–Biceranoæ•°æ®é›†: {len(df)}ä¸ªæ ·æœ¬")
            except FileNotFoundError:
                print("      âŒ æ‰¾ä¸åˆ°Biceranoæ•°æ®é›†æ–‡ä»¶")
                return pd.DataFrame(columns=['SMILES', 'Tg'])
            except Exception as e:
                print(f"      âŒ è¯»å–Biceranoæ•°æ®é›†å¤±è´¥: {e}")
                return pd.DataFrame(columns=['SMILES', 'Tg'])
            
            if 'Tg (K) exp' in df.columns and 'SMILES' in df.columns:
                # è½¬æ¢æ¸©åº¦å•ä½ä»Kåˆ°C
                df['Tg'] = df['Tg (K) exp'] - 273.15
                result_df = df[['SMILES', 'Tg']].dropna()
                print(f"      âœ… å¤„ç†å®Œæˆ: {len(result_df)}ä¸ªæœ‰æ•ˆTgæ ·æœ¬")
                return result_df
            else:
                print(f"      âš ï¸ Biceranoæ•°æ®é›†ç¼ºå°‘å¿…è¦åˆ—ï¼Œå¯ç”¨åˆ—: {list(df.columns)}")
                return pd.DataFrame(columns=['SMILES', 'Tg'])
        except Exception as e:
            print(f"      âŒ å¤„ç†Biceranoæ•°æ®é›†å¤±è´¥: {e}")
            return pd.DataFrame(columns=['SMILES', 'Tg'])
    
    def _process_extended_polymer_data(self, df):
        """å¤„ç†æ‰©å±•èšåˆç‰©æ•°æ®é›†ï¼ˆåŒ…å«å¤šä¸ªç›®æ ‡å˜é‡ï¼‰"""
        try:
            # ç›´æ¥è¯»å–æ–‡ä»¶
            try:
                df = pd.read_csv('newdata/extended_polymer_dataset.csv')
            except FileNotFoundError:
                print("      âŒ æ‰¾ä¸åˆ°æ‰©å±•æ•°æ®é›†æ–‡ä»¶")
                return pd.DataFrame(columns=['SMILES'])
            
            # è¿”å›åŒ…å«æ‰€æœ‰å¯ç”¨ç›®æ ‡å˜é‡çš„æ•°æ®
            available_targets = [col for col in ['Tg', 'Density', 'Tc'] if col in df.columns]
            if available_targets:
                print(f"      âœ… æ‰©å±•æ•°æ®é›†åŒ…å«ç›®æ ‡: {available_targets}")
                return df[['SMILES'] + available_targets].dropna(subset=['SMILES'])
            else:
                print("      âš ï¸ æ‰©å±•æ•°æ®é›†ç¼ºå°‘ç›®æ ‡å˜é‡")
                return pd.DataFrame(columns=['SMILES'])
        except Exception as e:
            print(f"      âŒ å¤„ç†æ‰©å±•æ•°æ®é›†å¤±è´¥: {e}")
            return pd.DataFrame(columns=['SMILES'])
    
    def _process_bimog_data(self, file_path_or_df):
        """å¤„ç†BIMOGæ•°æ®åº“æ•°æ®é›†"""
        try:
            # å¦‚æœä¼ å…¥çš„æ˜¯æ–‡ä»¶è·¯å¾„ï¼Œç›´æ¥è¯»å–æ–‡ä»¶
            if isinstance(file_path_or_df, str):
                try:
                    df = pd.read_csv(file_path_or_df, sep=';', encoding='utf-8', on_bad_lines='skip')
                    print(f"      âœ… æˆåŠŸè¯»å–BIMOGæ•°æ®é›†: {len(df)}ä¸ªæ ·æœ¬")
                except UnicodeDecodeError:
                    try:
                        df = pd.read_csv(file_path_or_df, sep=';', encoding='latin-1', on_bad_lines='skip')
                        print(f"      âœ… æˆåŠŸè¯»å–BIMOGæ•°æ®é›†(latin-1ç¼–ç ): {len(df)}ä¸ªæ ·æœ¬")
                    except Exception as e:
                        print(f"      âŒ è¯»å–BIMOGæ•°æ®é›†å¤±è´¥(ç¼–ç é—®é¢˜): {e}")
                        return pd.DataFrame(columns=['SMILES', 'Tg'])
                except FileNotFoundError:
                    print("      âŒ æ‰¾ä¸åˆ°BIMOGæ•°æ®é›†æ–‡ä»¶")
                    return pd.DataFrame(columns=['SMILES', 'Tg'])
                except Exception as e:
                    print(f"      âŒ è¯»å–BIMOGæ•°æ®é›†å¤±è´¥: {e}")
                    return pd.DataFrame(columns=['SMILES', 'Tg'])
            else:
                # å¦‚æœä¼ å…¥çš„æ˜¯DataFrameï¼Œç›´æ¥ä½¿ç”¨
                df = file_path_or_df
                print(f"      âœ… ä½¿ç”¨ä¼ å…¥çš„BIMOGæ•°æ®é›†: {len(df)}ä¸ªæ ·æœ¬")
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—
            if 'Tg / K' in df.columns and 'SMILES' in df.columns:
                # è½¬æ¢æ¸©åº¦å•ä½ä»Kåˆ°Cï¼Œå¹¶æ¸…ç†æ•°æ®
                df_clean = df[['SMILES', 'Tg / K']].copy()
                df_clean = df_clean.dropna(subset=['SMILES', 'Tg / K'])
                
                # è½¬æ¢æ¸©åº¦å•ä½
                df_clean['Tg'] = pd.to_numeric(df_clean['Tg / K'], errors='coerce') - 273.15
                
                # ç§»é™¤æ— æ•ˆçš„æ¸©åº¦å€¼
                df_clean = df_clean.dropna(subset=['Tg'])
                
                # å»é‡ï¼ˆåŸºäºSMILESï¼‰
                df_clean = df_clean.drop_duplicates(subset=['SMILES'])
                
                result_df = df_clean[['SMILES', 'Tg']]
                print(f"      âœ… å¤„ç†å®Œæˆ: {len(result_df)}ä¸ªæœ‰æ•ˆTgæ ·æœ¬ï¼ˆå»é‡åï¼‰")
                return result_df
            else:
                print(f"      âš ï¸ BIMOGæ•°æ®é›†ç¼ºå°‘å¿…è¦åˆ—ï¼Œå¯ç”¨åˆ—: {list(df.columns)}")
                return pd.DataFrame(columns=['SMILES', 'Tg'])
        except Exception as e:
             print(f"      âŒ å¤„ç†BIMOGæ•°æ®é›†å¤±è´¥: {e}")
             return pd.DataFrame(columns=['SMILES', 'Tg'])
    
    def add_external_data_clean(self, df_train, df_extra, target):
        """æ·»åŠ å¤–éƒ¨æ•°æ®ï¼ˆå®Œå…¨å‚è€ƒé“œç‰Œæ–¹æ¡ˆçš„add_extra_data_cleanå‡½æ•°ï¼‰"""
        # å¤„ç†åªæœ‰SMILESæ²¡æœ‰ç›®æ ‡å˜é‡çš„æƒ…å†µ
        if target == 'SMILES_ONLY':
            print(f"      å¤„ç† {len(df_extra)} ä¸ªçº¯SMILESæ ·æœ¬...")
            
            # æ¸…æ´—å¤–éƒ¨SMILES
            df_extra['SMILES'] = df_extra['SMILES'].apply(self.clean_and_validate_smiles)
            
            # ç§»é™¤æ— æ•ˆSMILES
            before_filter = len(df_extra)
            df_extra = df_extra[df_extra['SMILES'].notnull()]
            after_filter = len(df_extra)
            
            print(f"      ä¿ç•™ {after_filter}/{before_filter} ä¸ªæœ‰æ•ˆSMILES")
            
            if len(df_extra) == 0:
                print(f"      æ²¡æœ‰å‰©ä½™æœ‰æ•ˆSMILESæ•°æ®")
                return df_train
            
            # æ‰¾åˆ°å”¯ä¸€SMILES
            unique_smiles_extra = set(df_extra['SMILES']) - set(df_train['SMILES'])
            
            # æ·»åŠ å”¯ä¸€SMILES
            if len(unique_smiles_extra) > 0:
                new_rows = []
                for smiles in unique_smiles_extra:
                    new_row = {'SMILES': smiles}
                    for col in self.task_names:
                        new_row[col] = np.nan
                    new_rows.append(new_row)
                
                extra_to_add = pd.DataFrame(new_rows)
                df_train = pd.concat([df_train, extra_to_add], axis=0, ignore_index=True)
            
            print(f'      æ·»åŠ äº† {len(unique_smiles_extra)} ä¸ªå”¯ä¸€SMILES')
            return df_train
        
        # åŸæœ‰çš„å¤„ç†æœ‰ç›®æ ‡å˜é‡çš„é€»è¾‘
        n_samples_before = len(df_train[df_train[target].notnull()])
        
        print(f"      å¤„ç† {len(df_extra)} ä¸ª {target} æ ·æœ¬...")
        
        # æ¸…æ´—å¤–éƒ¨SMILES
        df_extra['SMILES'] = df_extra['SMILES'].apply(self.clean_and_validate_smiles)
        
        # ç§»é™¤æ— æ•ˆSMILESå’Œç¼ºå¤±ç›®æ ‡å€¼
        before_filter = len(df_extra)
        df_extra = df_extra[df_extra['SMILES'].notnull()]
        df_extra = df_extra.dropna(subset=[target])
        after_filter = len(df_extra)
        
        print(f"      ä¿ç•™ {after_filter}/{before_filter} ä¸ªæœ‰æ•ˆæ ·æœ¬")
        
        if len(df_extra) == 0:
            print(f"      {target} æ²¡æœ‰å‰©ä½™æœ‰æ•ˆæ•°æ®")
            return df_train
        
        # æŒ‰è§„èŒƒSMILESåˆ†ç»„å¹¶å¹³å‡é‡å¤å€¼
        df_extra = df_extra.groupby('SMILES', as_index=False)[target].mean()
        
        # æ‰¾åˆ°äº¤é›†å’Œå”¯ä¸€SMILES
        cross_smiles = set(df_extra['SMILES']) & set(df_train['SMILES'])
        unique_smiles_extra = set(df_extra['SMILES']) - set(df_train['SMILES'])
        
        # å¡«å……ç¼ºå¤±å€¼
        filled_count = 0
        for smile in df_train[df_train[target].isnull()]['SMILES'].tolist():
            if smile in cross_smiles:
                df_train.loc[df_train['SMILES']==smile, target] = \
                    df_extra[df_extra['SMILES']==smile][target].values[0]
                filled_count += 1
        
        # æ·»åŠ å”¯ä¸€SMILES
        extra_to_add = df_extra[df_extra['SMILES'].isin(unique_smiles_extra)].copy()
        if len(extra_to_add) > 0:
            for col in self.task_names:
                if col not in extra_to_add.columns:
                    extra_to_add[col] = np.nan
            
            extra_to_add = extra_to_add[['SMILES'] + self.task_names]
            df_train = pd.concat([df_train, extra_to_add], axis=0, ignore_index=True)
        
        n_samples_after = len(df_train[df_train[target].notnull()])
        print(f'      {target}: +{n_samples_after-n_samples_before} æ ·æœ¬, +{len(unique_smiles_extra)} å”¯ä¸€SMILES')
        
        return df_train
    
    def separate_subtables(self, df):
        """åˆ†ç¦»å„ä»»åŠ¡çš„å­è¡¨"""
        subtables = {}
        for task in self.task_names:
            mask = df[task].notna()
            subtables[task] = df[mask].copy()
        return subtables
    

    def prepare_task_features(self, task):
        """ä¸ºç‰¹å®šä»»åŠ¡å‡†å¤‡ç‰¹å¾"""
        task_data = self.subtables[task]
        smiles_list = task_data['SMILES'].tolist()
        y = task_data[task].values
        
        # ç‰¹å¾æå–
        fingerprints, descriptors, valid_smiles, invalid_indices = self.smiles_to_combined_features(
            smiles_list, self.task_filters[task], radius=2, n_bits=128
        )
        
        # å¤„ç†æè¿°ç¬¦
        X_desc = pd.DataFrame(descriptors)
        y = np.delete(y, invalid_indices)
        
        # è¿‡æ»¤ç‰¹å¾
        X_desc = X_desc.filter(self.task_filters[task])
        
        # ç»„åˆæŒ‡çº¹å’Œæè¿°ç¬¦
        fp_df = pd.DataFrame(fingerprints, columns=[f'FP_{i}' for i in range(fingerprints.shape[1])])
        fp_df.reset_index(drop=True, inplace=True)
        X_desc.reset_index(drop=True, inplace=True)
        
        X = pd.concat([X_desc, fp_df], axis=1)
        
        print(f"   ç‰¹å¾ç»„åˆå: {X.shape}")
        
        # æ–¹å·®é˜ˆå€¼è¿‡æ»¤ï¼ˆé™ä½é˜ˆå€¼ï¼‰
        threshold = 0.0001  # é™ä½é˜ˆå€¼
        selector = VarianceThreshold(threshold=threshold)
        
        try:
            X_filtered = selector.fit_transform(X)
            print(f"   æ–¹å·®è¿‡æ»¤å: {X_filtered.shape}")
        except ValueError:
            # å¦‚æœæ‰€æœ‰ç‰¹å¾éƒ½è¢«è¿‡æ»¤æ‰ï¼Œåˆ™ä¸è¿›è¡Œæ–¹å·®è¿‡æ»¤
            print("   è­¦å‘Š: æ–¹å·®è¿‡æ»¤å¤±è´¥ï¼Œè·³è¿‡æ–¹å·®è¿‡æ»¤æ­¥éª¤")
            X_filtered = X.values
            selector = None
        
        # æ•°æ®å¢å¼ºï¼ˆGMMï¼‰
        n_samples = 1000
        augmented_data = self.augment_dataset_with_gmm(X_filtered, y, n_samples=n_samples)
        
        print(f"   GMMå¢å¼ºå: {augmented_data.shape}")
        
        # ä¿å­˜ç‰¹å¾é€‰æ‹©å™¨
        self.feature_selectors[task] = selector
        
        return augmented_data
    
    def prepare_test_features(self, task):
        """ä¸ºç‰¹å®šä»»åŠ¡å‡†å¤‡æµ‹è¯•ç‰¹å¾"""
        test_smiles = self.test_df['SMILES'].tolist()

        # ç‰¹å¾æå–
        fingerprints, descriptors, valid_smiles, invalid_indices = self.smiles_to_combined_features(
            test_smiles, self.task_filters[task], radius=2, n_bits=128
        )

        # å¤„ç†æè¿°ç¬¦
        X_desc = pd.DataFrame(descriptors)
        X_desc = X_desc.filter(self.task_filters[task])

        # ç»„åˆæŒ‡çº¹å’Œæè¿°ç¬¦
        fp_df = pd.DataFrame(fingerprints, columns=[f'FP_{i}' for i in range(fingerprints.shape[1])])
        fp_df.reset_index(drop=True, inplace=True)
        X_desc.reset_index(drop=True, inplace=True)

        X = pd.concat([X_desc, fp_df], axis=1)

        # ç¡®ä¿æµ‹è¯•ç‰¹å¾ä¸è®­ç»ƒç‰¹å¾ç»´åº¦ä¸€è‡´
        print(f"   ğŸ” æµ‹è¯•ç‰¹å¾é¢„å¤„ç†: {X.shape}")

        # åŠ¨æ€è·å–è®­ç»ƒæ—¶çš„ç‰¹å¾æ•°é‡
        expected_features = None

        # å°è¯•ä»å·²è®­ç»ƒçš„æ¨¡å‹ä¸­è·å–ç‰¹å¾æ•°é‡
        if hasattr(self, 'models') and task in self.models and len(self.models[task]) > 0:
            xgb_model = self.models[task][0]
            if hasattr(xgb_model, 'n_features_in_'):
                expected_features = xgb_model.n_features_in_
                print(f"   ğŸ“Š ä»XGBoostæ¨¡å‹è·å–æœŸæœ›ç‰¹å¾æ•°: {expected_features}")
            elif hasattr(xgb_model, 'get_booster'):
                try:
                    booster = xgb_model.get_booster()
                    if hasattr(booster, 'num_features'):
                        expected_features = booster.num_features()
                        print(f"   ğŸ“Š ä»XGBoost boosterè·å–æœŸæœ›ç‰¹å¾æ•°: {expected_features}")
                except:
                    pass

        # å¦‚æœæ— æ³•è·å–ï¼Œå°è¯•ä»ç‰¹å¾é€‰æ‹©å™¨æ¨æ–­
        if expected_features is None:
            if hasattr(self, 'feature_selectors') and task in self.feature_selectors:
                if self.feature_selectors[task] is not None:
                    if hasattr(self.feature_selectors[task], 'n_features_in_'):
                        base_features = self.feature_selectors[task].n_features_in_
                        expected_features = base_features
                        print(f"   ğŸ“Š ä»ç‰¹å¾é€‰æ‹©å™¨æ¨æ–­æœŸæœ›ç‰¹å¾æ•°: {expected_features}")

        # å¦‚æœä»ç„¶æ— æ³•è·å–ï¼Œä½¿ç”¨ä¿å®ˆä¼°è®¡
        if expected_features is None:
            expected_features = 1603  # ä»é”™è¯¯ä¿¡æ¯ä¸­çœ‹åˆ°çš„å®é™…éœ€è¦çš„ç‰¹å¾æ•°
            print(f"   ğŸ“Š ä½¿ç”¨ä¿å®ˆä¼°è®¡çš„æœŸæœ›ç‰¹å¾æ•°: {expected_features}")

        # å¦‚æœæµ‹è¯•ç‰¹å¾æ•°é‡ä¸è®­ç»ƒæ—¶ä¸ä¸€è‡´ï¼Œéœ€è¦è°ƒæ•´
        if X.shape[1] != expected_features:
            print(f"   âš ï¸ ç‰¹å¾æ•°é‡ä¸åŒ¹é…: æœŸæœ›{expected_features}, å®é™…{X.shape[1]}")
            print(f"   ğŸ”„ è°ƒæ•´æµ‹è¯•ç‰¹å¾ä»¥åŒ¹é…è®­ç»ƒæ—¶çš„ç»´åº¦...")

            # å¦‚æœæµ‹è¯•ç‰¹å¾å°‘äºè®­ç»ƒç‰¹å¾ï¼Œç”¨é›¶å¡«å……
            if X.shape[1] < expected_features:
                padding = np.zeros((X.shape[0], expected_features - X.shape[1]))
                X_padded = np.concatenate([X.values, padding], axis=1)
                print(f"   âœ… ç‰¹å¾å¡«å……: {X.shape[1]} â†’ {expected_features}")
                return X_padded

            # å¦‚æœæµ‹è¯•ç‰¹å¾å¤šäºè®­ç»ƒç‰¹å¾ï¼Œæˆªå–å‰Nä¸ª
            elif X.shape[1] > expected_features:
                X_truncated = X.values[:, :expected_features]
                print(f"   âœ… ç‰¹å¾æˆªå–: {X.shape[1]} â†’ {expected_features}")
                return X_truncated

        # æ­£å¸¸çš„ç‰¹å¾é€‰æ‹©æµç¨‹
        # é¦–å…ˆåº”ç”¨é¢‘æ¬¡ç‰¹å¾é€‰æ‹©å™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if hasattr(self, 'frequency_selectors') and task in self.frequency_selectors:
            freq_selected_features = self.frequency_selectors[task]

            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦è¶Šç•Œ
            max_index = max(freq_selected_features) if freq_selected_features else 0
            if max_index >= X.shape[1]:
                print(f"   âš ï¸ é¢‘æ¬¡é€‰æ‹©å™¨ç´¢å¼•è¶Šç•Œï¼Œè·³è¿‡é¢‘æ¬¡è¿‡æ»¤")
                X_freq_filtered = X.values
            else:
                X_freq_filtered = X.values[:, freq_selected_features]
        else:
            X_freq_filtered = X.values

        print(f"   âœ… é¢‘æ¬¡è¿‡æ»¤åç‰¹å¾å½¢çŠ¶: {X_freq_filtered.shape}")

        # ç„¶ååº”ç”¨æ–¹å·®ç‰¹å¾é€‰æ‹©å™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if hasattr(self, 'feature_selectors') and task in self.feature_selectors:
            if self.feature_selectors[task] is not None:
                try:
                    X_filtered = self.feature_selectors[task].transform(X_freq_filtered)
                    print(f"   âœ… æ–¹å·®è¿‡æ»¤åç‰¹å¾å½¢çŠ¶: {X_filtered.shape}")
                except Exception as e:
                    print(f"   âš ï¸ æ–¹å·®è¿‡æ»¤å¤±è´¥: {e}")
                    X_filtered = X_freq_filtered
            else:
                X_filtered = X_freq_filtered
        else:
            X_filtered = X_freq_filtered

        print(f"   ğŸ¯ æœ€ç»ˆæµ‹è¯•ç‰¹å¾å½¢çŠ¶: {X_filtered.shape}")
        return X_filtered
    
    def get_task_specific_xgb_params(self, task):
        """è·å–ä»»åŠ¡ç‰¹å®šçš„XGBoostå‚æ•°ï¼ˆä¸é“œç‰Œæ–¹æ¡ˆå®Œå…¨ä¸€è‡´ï¼‰"""
        # ä¸é“œç‰Œæ–¹æ¡ˆå®Œå…¨ä¸€è‡´çš„å‚æ•°è®¾ç½®
        if task == "Tg":
            return {
                'n_estimators': 2200, 
                'learning_rate': 0.06584519841235120, 
                'max_depth': 6, 
                'reg_lambda': 5.545520219149715, 
                'random_state': 4
            }
        elif task == "Rg":
            return {
                'n_estimators': 520, 
                'learning_rate': 0.07324113948440986, 
                'max_depth': 5, 
                'reg_lambda': 0.9717380315982088, 
                'random_state': 4
            }
        elif task == "FFV":
            return {
                'n_estimators': 2202, 
                'learning_rate': 0.07220580588586338, 
                'max_depth': 4, 
                'reg_lambda': 2.8872976032666493, 
                'random_state': 4
            }
        elif task == "Tc":
            return {
                'n_estimators': 1488, 
                'learning_rate': 0.010456188013762864, 
                'max_depth': 5, 
                'reg_lambda': 9.970345982204618, 
                'random_state': 4
            }
        elif task == "Density":
            return {
                'n_estimators': 1958, 
                'learning_rate': 0.10955287548172478, 
                'max_depth': 5, 
                'reg_lambda': 3.074470087965767, 
                'random_state': 4
            }
        else:
            # é»˜è®¤å‚æ•°
            return {
                'n_estimators': 500,
                'learning_rate': 0.1,
                'max_depth': 6,
                'random_state': 42
            }
    
    def _get_ml_model_path(self, task, model_type):
        """è·å–æœºå™¨å­¦ä¹ æ¨¡å‹æ–‡ä»¶è·¯å¾„"""
        if self.model_path:
            # ä½¿ç”¨è‡ªå®šä¹‰è·¯å¾„
            model_dir = self.model_path
        else:
            # ä½¿ç”¨é»˜è®¤è·¯å¾„
            model_dir = os.path.join(os.getcwd(), 'saved_models')
        os.makedirs(model_dir, exist_ok=True)
        return os.path.join(model_dir, f'{task}_{model_type}_ml_model.pkl')
    
    def _save_ml_models(self, task, xgb_model, rf_model, feature_selector=None):
        """ä¿å­˜æœºå™¨å­¦ä¹ æ¨¡å‹å’Œç›¸å…³ç»„ä»¶"""
        try:
            # ä¿å­˜XGBoostæ¨¡å‹
            xgb_path = self._get_ml_model_path(task, 'xgb')
            joblib.dump(xgb_model, xgb_path)

            # ä¿å­˜RandomForestæ¨¡å‹
            rf_path = self._get_ml_model_path(task, 'rf')
            joblib.dump(rf_model, rf_path)

            # ä¿å­˜ç‰¹å¾é€‰æ‹©å™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if feature_selector is not None:
                selector_path = self._get_ml_model_path(task, 'selector')
                joblib.dump(feature_selector, selector_path)

            # ä¿å­˜é¢‘æ¬¡é€‰æ‹©å™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if hasattr(self, 'frequency_selectors') and task in self.frequency_selectors:
                freq_selector_path = self._get_ml_model_path(task, 'freq_selector')
                joblib.dump(self.frequency_selectors[task], freq_selector_path)

            print(f"    âœ… {task}ä»»åŠ¡çš„MLæ¨¡å‹å·²ä¿å­˜")
            return True
        except Exception as e:
            print(f"    âŒ ä¿å­˜{task}ä»»åŠ¡MLæ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def _load_ml_models(self, task):
        """åŠ è½½æœºå™¨å­¦ä¹ æ¨¡å‹å’Œç›¸å…³ç»„ä»¶"""
        try:
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            xgb_path = self._get_ml_model_path(task, 'xgb')
            rf_path = self._get_ml_model_path(task, 'rf')
            selector_path = self._get_ml_model_path(task, 'selector')
            
            if not (os.path.exists(xgb_path) and os.path.exists(rf_path)):
                return None, None, None
            
            # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§ï¼ˆæ£€æŸ¥æ–‡ä»¶å¤§å°ï¼‰
            if os.path.getsize(xgb_path) == 0 or os.path.getsize(rf_path) == 0:
                print(f"    âš ï¸ {task}ä»»åŠ¡çš„æ¨¡å‹æ–‡ä»¶æŸåï¼ˆæ–‡ä»¶å¤§å°ä¸º0ï¼‰ï¼Œå°†é‡æ–°è®­ç»ƒ")
                return None, None, None
            
            # åŠ è½½æ¨¡å‹
            xgb_model = joblib.load(xgb_path)
            rf_model = joblib.load(rf_path)
            
            # éªŒè¯æ¨¡å‹å¯¹è±¡çš„æœ‰æ•ˆæ€§
            if not hasattr(xgb_model, 'predict') or not hasattr(rf_model, 'predict'):
                print(f"    âš ï¸ {task}ä»»åŠ¡çš„æ¨¡å‹å¯¹è±¡æ— æ•ˆï¼Œå°†é‡æ–°è®­ç»ƒ")
                return None, None, None
            
            # åŠ è½½ç‰¹å¾é€‰æ‹©å™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            feature_selector = None
            if os.path.exists(selector_path):
                if os.path.getsize(selector_path) > 0:
                    feature_selector = joblib.load(selector_path)
                    # éªŒè¯ç‰¹å¾é€‰æ‹©å™¨
                    if not hasattr(feature_selector, 'transform'):
                        print(f"    âš ï¸ {task}ä»»åŠ¡çš„ç‰¹å¾é€‰æ‹©å™¨æ— æ•ˆ")
                        feature_selector = None

            # åŠ è½½é¢‘æ¬¡é€‰æ‹©å™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            freq_selector_path = self._get_ml_model_path(task, 'freq_selector')
            if os.path.exists(freq_selector_path):
                if os.path.getsize(freq_selector_path) > 0:
                    freq_selector = joblib.load(freq_selector_path)
                    if not hasattr(self, 'frequency_selectors'):
                        self.frequency_selectors = {}
                    self.frequency_selectors[task] = freq_selector

            print(f"    âœ… {task}ä»»åŠ¡çš„MLæ¨¡å‹å·²åŠ è½½")
            return xgb_model, rf_model, feature_selector
        except Exception as e:
            error_msg = str(e)
            print(f"    âŒ åŠ è½½{task}ä»»åŠ¡MLæ¨¡å‹å¤±è´¥: {error_msg}")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºscikit-learnç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
            if "node array from the pickle has an incompatible dtype" in error_msg or "incompatible dtype" in error_msg:
                print(f"    ğŸ”§ æ£€æµ‹åˆ°scikit-learnç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜")
                print(f"    ğŸ“‹ å»ºè®®ï¼šè¯·ä½¿ç”¨ç›¸åŒç‰ˆæœ¬çš„scikit-learné‡æ–°è®­ç»ƒå¹¶ä¸Šä¼ æ¨¡å‹æ–‡ä»¶")
                print(f"    ğŸ”„ å½“å‰å°†è·³è¿‡æ¨¡å‹åŠ è½½ï¼Œé‡æ–°è®­ç»ƒæ–°æ¨¡å‹")
                # åœ¨Kaggleç¯å¢ƒä¸­ï¼Œé€šå¸¸æ— æ³•åˆ é™¤è¾“å…¥ç›®å½•ä¸­çš„æ–‡ä»¶ï¼Œæ‰€ä»¥ç›´æ¥è·³è¿‡åˆ é™¤æ“ä½œ
            
            return None, None, None
    
    def train_task_specific_models(self):
        """è®­ç»ƒä»»åŠ¡ç‰¹å®šçš„æ¨¡å‹"""
        print("ğŸ¤– å¼€å§‹è®­ç»ƒä»»åŠ¡ç‰¹å®šæ¨¡å‹...")
        
        predictions_df = pd.DataFrame({'id': self.test_df['id']})
        mae_scores = {}
        # æ”¶é›†äº¤å‰éªŒè¯çš„çœŸå®å€¼å’Œé¢„æµ‹å€¼ç”¨äºç«èµ›wMAEè®¡ç®—
        cv_true_values = {}
        cv_pred_values = {}
        
        for task in self.task_names:
            print(f"\nğŸ“ˆ è®­ç»ƒä»»åŠ¡: {task}")
            
            # å°è¯•åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹ï¼ˆä»…åœ¨å¯ç”¨æ—¶ï¼‰
            if self.use_saved_models:
                xgb_model, rf_model, saved_selector = self._load_ml_models(task)

                if xgb_model is not None and rf_model is not None:
                    print(f"    ğŸ”„ ä½¿ç”¨å·²ä¿å­˜çš„{task}æ¨¡å‹ï¼Œè·³è¿‡è®­ç»ƒ")
            else:
                print(f"    ğŸš« å·²ä¿å­˜æ¨¡å‹åŠŸèƒ½å·²ç¦ç”¨ï¼Œå°†é‡æ–°è®­ç»ƒ{task}æ¨¡å‹")
                xgb_model, rf_model, saved_selector = None, None, None

            if xgb_model is not None and rf_model is not None and self.use_saved_models:
                
                # å‡†å¤‡æµ‹è¯•ç‰¹å¾ï¼ˆéœ€è¦ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾å¤„ç†æµç¨‹ï¼‰
                test_features = self.prepare_test_features(task)
                
                # å¦‚æœæœ‰ä¿å­˜çš„ç‰¹å¾é€‰æ‹©å™¨ï¼Œåº”ç”¨åˆ°æµ‹è¯•ç‰¹å¾
                if saved_selector is not None:
                    test_features = saved_selector.transform(test_features)
                
                # ä½¿ç”¨åŠ è½½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
                xgb_test_preds = xgb_model.predict(test_features)
                rf_test_preds = rf_model.predict(test_features)
                final_test_preds = (xgb_test_preds + rf_test_preds) / 2
                
                predictions_df[task] = final_test_preds
                mae_scores[task] = 0.0  # æ— æ³•è®¡ç®—CVåˆ†æ•°ï¼Œè®¾ä¸º0
                
                # å­˜å‚¨ç‰¹å¾é€‰æ‹©å™¨ä»¥ä¾›åç»­ä½¿ç”¨
                if saved_selector is not None:
                    self.feature_selectors[task] = saved_selector
                
                continue
            
            # è·å–ä»»åŠ¡ç‰¹å®šæ•°æ®
            task_data = self.subtables[task]
            print(f"   ä»»åŠ¡æ•°æ®å½¢çŠ¶: {task_data.shape}")
            
            if len(task_data) == 0:
                print(f"âš ï¸ ä»»åŠ¡{task}æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡è®­ç»ƒ")
                predictions_df[task] = np.zeros(len(self.test_df))
                mae_scores[task] = 999.0
                continue
            
            # SMILESå¢å¼ºï¼ˆå‚è€ƒé“œç‰Œæ–¹æ¡ˆï¼Œä½¿ç”¨ç›¸åŒå¢å¼ºå€æ•°ï¼‰
            original_smiles = task_data['SMILES'].tolist()
            original_labels = task_data[task].values
            
            augmented_smiles, augmented_labels = self.augment_smiles_dataset(
                original_smiles, original_labels, num_augments=1  # ä¸é“œç‰Œæ–¹æ¡ˆä¸€è‡´
            )
            
            # ç‰¹å¾æå–
            fingerprints, descriptors, valid_smiles, invalid_indices = self.smiles_to_combined_features(
                augmented_smiles, self.task_filters[task], radius=2, n_bits=128
            )

            # éªŒè¯ç‰¹å¾æå–ç»“æœ
            if len(fingerprints) == 0:
                print(f"   âŒ {task}ä»»åŠ¡ç‰¹å¾æå–å¤±è´¥ï¼Œè·³è¿‡è®­ç»ƒ")
                predictions_df[task] = np.zeros(len(self.test_df))
                mae_scores[task] = 999.0
                continue
            
            # å¤„ç†æè¿°ç¬¦
            X_desc = pd.DataFrame(descriptors)
            y = np.delete(augmented_labels, invalid_indices)

            # éªŒè¯æ•°æ®è´¨é‡
            if len(y) == 0:
                print(f"   âŒ {task}ä»»åŠ¡æ²¡æœ‰æœ‰æ•ˆçš„æ ‡ç­¾æ•°æ®ï¼Œè·³è¿‡è®­ç»ƒ")
                predictions_df[task] = np.zeros(len(self.test_df))
                mae_scores[task] = 999.0
                continue

            # æ£€æŸ¥æ ‡ç­¾æ•°æ®çš„åˆç†æ€§
            y_clean = y[~np.isnan(y)]
            if len(y_clean) == 0:
                print(f"   âŒ {task}ä»»åŠ¡æ ‡ç­¾å…¨ä¸ºNaNï¼Œè·³è¿‡è®­ç»ƒ")
                predictions_df[task] = np.zeros(len(self.test_df))
                mae_scores[task] = 999.0
                continue

            print(f"   ğŸ“Š {task}æ ‡ç­¾èŒƒå›´: {np.min(y_clean):.3f} ~ {np.max(y_clean):.3f}")

            # è¿‡æ»¤ç‰¹å¾
            X_desc = X_desc.filter(self.task_filters[task])
            
            # æ·»åŠ æŒ‡çº¹ç‰¹å¾
            if fingerprints.shape[1] == 0:
                print(f"   âŒ {task}ä»»åŠ¡æ²¡æœ‰æœ‰æ•ˆæŒ‡çº¹ç‰¹å¾ï¼Œè·³è¿‡è®­ç»ƒ")
                predictions_df[task] = np.zeros(len(self.test_df))
                mae_scores[task] = 999.0
                continue

            fp_df = pd.DataFrame(fingerprints, columns=[f'FP_{i}' for i in range(fingerprints.shape[1])])
            fp_df.reset_index(drop=True, inplace=True)
            X_desc.reset_index(drop=True, inplace=True)
            X = pd.concat([X_desc, fp_df], axis=1)

            print(f"   åˆå¹¶åç‰¹å¾å½¢çŠ¶: {X.shape}")

            # æ£€æŸ¥ç‰¹å¾æ•°æ®è´¨é‡
            if X.shape[1] == 0:
                print(f"   âŒ {task}ä»»åŠ¡æ²¡æœ‰æœ‰æ•ˆç‰¹å¾ï¼Œè·³è¿‡è®­ç»ƒ")
                predictions_df[task] = np.zeros(len(self.test_df))
                mae_scores[task] = 999.0
                continue
            
            # Githubé¡¹ç›®å¯å‘çš„é¢‘æ¬¡ç‰¹å¾é€‰æ‹©
            try:
                X_freq_filtered, freq_selected_features = self.apply_frequency_based_feature_selection(
                    X.values, list(X.columns)
                )
                print(f"   é¢‘æ¬¡è¿‡æ»¤åç‰¹å¾å½¢çŠ¶: {X_freq_filtered.shape}")
                # ä¿å­˜é¢‘æ¬¡é€‰æ‹©å™¨
                self.frequency_selectors[task] = freq_selected_features
            except Exception as e:
                print(f"   é¢‘æ¬¡è¿‡æ»¤å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç‰¹å¾: {e}")
                X_freq_filtered = X.values
                freq_selected_features = list(range(X.shape[1]))
                self.frequency_selectors[task] = freq_selected_features

            # æ–¹å·®è¿‡æ»¤ï¼ˆåœ¨é¢‘æ¬¡è¿‡æ»¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥ä¼˜åŒ–ï¼‰
            threshold = 0.01  # æé«˜é˜ˆå€¼ï¼Œä¸é“œç‰Œæ–¹æ¡ˆä¸€è‡´
            try:
                selector = VarianceThreshold(threshold=threshold)
                X_filtered = selector.fit_transform(X_freq_filtered)
                print(f"   æ–¹å·®è¿‡æ»¤åç‰¹å¾å½¢çŠ¶: {X_filtered.shape}")
                self.feature_selectors[task] = selector
            except ValueError as e:
                print(f"   æ–¹å·®è¿‡æ»¤å¤±è´¥ï¼Œä½¿ç”¨é¢‘æ¬¡è¿‡æ»¤ç»“æœ: {e}")
                X_filtered = X_freq_filtered
                self.feature_selectors[task] = None
            
            # GMMæ•°æ®å¢å¼ºï¼ˆå‚è€ƒé“œç‰Œæ–¹æ¡ˆï¼Œå›ºå®šå‚æ•°ï¼‰
            if len(X_filtered) > 10:
                try:
                    # ä½¿ç”¨é“œç‰Œæ–¹æ¡ˆçš„å›ºå®šå‚æ•°
                    n_samples = 1000  # å›ºå®šæ ·æœ¬æ•°ï¼Œä¸é“œç‰Œæ–¹æ¡ˆä¸€è‡´
                    n_components = 5   # å›ºå®šç»„ä»¶æ•°ï¼Œä¸é“œç‰Œæ–¹æ¡ˆä¸€è‡´
                    
                    augmented_data = self.augment_dataset_with_gmm(
                        X_filtered, y, n_samples=n_samples, n_components=n_components
                    )
                    X_final = augmented_data.drop(columns=['Target']).values
                    y_final = augmented_data['Target'].values
                    print(f"   GMMå¢å¼ºåæ•°æ®å½¢çŠ¶: {X_final.shape}")
                except Exception as e:
                    print(f"   GMMå¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®: {e}")
                    X_final = X_filtered
                    y_final = y
            else:
                X_final = X_filtered
                y_final = y
            
            # å‡†å¤‡æµ‹è¯•ç‰¹å¾
            test_features = self.prepare_test_features(task)
            
            # äº¤å‰éªŒè¯è®­ç»ƒ
            if len(y_final) < 5:
                print(f"   æ ·æœ¬æ•°é‡å¤ªå°‘({len(y_final)})ï¼Œè·³è¿‡äº¤å‰éªŒè¯")
                predictions_df[task] = np.zeros(len(self.test_df))
                mae_scores[task] = 999.0
                continue
                
            kf = KFold(n_splits=5, shuffle=True, random_state=42)
            oof_predictions = np.zeros(len(y_final))
            test_predictions = np.zeros((5, len(test_features)))
            fold_scores = []
            
            for fold, (train_idx, val_idx) in enumerate(kf.split(X_final)):
                print(f"    æŠ˜{fold + 1}/5")
                
                X_train_fold = X_final[train_idx]
                X_val_fold = X_final[val_idx]
                y_train_fold = y_final[train_idx]
                y_val_fold = y_final[val_idx]
                
                # XGBoostæ¨¡å‹ï¼ˆå‚è€ƒé“œç‰Œæ–¹æ¡ˆå‚æ•°ï¼‰
                xgb_params = self.get_task_specific_xgb_params(task)
                xgb_model = XGBRegressor(**xgb_params)
                xgb_model.fit(X_train_fold, y_train_fold, 
                             eval_set=[(X_val_fold, y_val_fold)], verbose=False)
                xgb_oof_preds = xgb_model.predict(X_val_fold)
                xgb_test_preds = xgb_model.predict(test_features)
                
                # RandomForestæ¨¡å‹ï¼ˆå‚è€ƒé“œç‰Œæ–¹æ¡ˆå‚æ•°ï¼‰
                rf_model = RandomForestRegressor(
                    random_state=42  # ä¸é“œç‰Œæ–¹æ¡ˆå®Œå…¨ä¸€è‡´
                )
                rf_model.fit(X_train_fold, y_train_fold)
                rf_oof_preds = rf_model.predict(X_val_fold)
                rf_test_preds = rf_model.predict(test_features)
                
                # é›†æˆé¢„æµ‹ï¼ˆä¸¤æ¨¡å‹å¹³å‡ï¼Œä¸é“œç‰Œæ–¹æ¡ˆä¸€è‡´ï¼‰
                fold_oof_preds = (xgb_oof_preds + rf_oof_preds) / 2
                fold_test_preds = (xgb_test_preds + rf_test_preds) / 2
                
                # ä¿å­˜é¢„æµ‹
                oof_predictions[val_idx] = fold_oof_preds
                test_predictions[fold] = fold_test_preds
                
                # è®¡ç®—æŠ˜å¾—åˆ†
                fold_mae = mean_absolute_error(y_val_fold, fold_oof_preds)
                fold_scores.append(fold_mae)
                print(f"      æŠ˜{fold + 1} MAE: {fold_mae:.4f}")
            
            # è®¡ç®—CVå¾—åˆ† - æ·»åŠ MAPEæŒ‡æ ‡ï¼ˆGithubé¡¹ç›®å¯å‘ï¼‰
            cv_mae = mean_absolute_error(y_final, oof_predictions)
            cv_mape = mean_absolute_percentage_error(y_final, oof_predictions)
            cv_std = np.std(fold_scores)
            mae_scores[task] = cv_mae

            # æ”¶é›†äº¤å‰éªŒè¯ç»“æœç”¨äºç«èµ›wMAEè®¡ç®—
            cv_true_values[task] = y_final.copy()
            cv_pred_values[task] = oof_predictions.copy()

            print(f"    {task} CV MAE: {cv_mae:.4f} Â± {cv_std:.4f}")
            print(f"    {task} CV MAPE: {cv_mape:.2f}%")
            
            # æœ€ç»ˆæµ‹è¯•é¢„æµ‹
            final_test_preds = np.mean(test_predictions, axis=0)

            # ğŸ”§ é¢„æµ‹ç»“æœéªŒè¯å’Œä¿®å¤
            if np.all(final_test_preds == 0) or np.all(np.isnan(final_test_preds)):
                print(f"    âš ï¸ {task}é¢„æµ‹ç»“æœå…¨ä¸º0æˆ–NaNï¼Œä½¿ç”¨è®­ç»ƒæ•°æ®å‡å€¼å¡«å……")
                # ä½¿ç”¨è®­ç»ƒæ•°æ®çš„å‡å€¼ä½œä¸ºå¤‡ç”¨é¢„æµ‹
                train_mean = np.nanmean(y_final)
                if np.isnan(train_mean):
                    # å¦‚æœè®­ç»ƒæ•°æ®å‡å€¼ä¹Ÿæ˜¯NaNï¼Œä½¿ç”¨ä»»åŠ¡ç‰¹å®šçš„é»˜è®¤å€¼
                    default_values = {
                        'Tg': 100.0,    # å…¸å‹Tgå€¼
                        'FFV': 0.35,    # å…¸å‹FFVå€¼
                        'Tc': 0.25,     # å…¸å‹Tcå€¼
                        'Density': 1.0, # å…¸å‹å¯†åº¦å€¼
                        'Rg': 15.0      # å…¸å‹Rgå€¼
                    }
                    train_mean = default_values.get(task, 0.0)
                final_test_preds = np.full(len(self.test_df), train_mean)
                print(f"    ğŸ”§ ä½¿ç”¨å¤‡ç”¨å€¼: {train_mean:.3f}")

            # æ£€æŸ¥é¢„æµ‹èŒƒå›´çš„åˆç†æ€§
            pred_min, pred_max = np.min(final_test_preds), np.max(final_test_preds)
            pred_std = np.std(final_test_preds)
            print(f"    ğŸ“Š {task}é¢„æµ‹èŒƒå›´: {pred_min:.3f} ~ {pred_max:.3f} (std: {pred_std:.3f})")

            predictions_df[task] = final_test_preds
            
            # è®­ç»ƒæœ€ç»ˆæ¨¡å‹ç”¨äºä¿å­˜ï¼ˆä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰
            print(f"    ğŸ’¾ è®­ç»ƒå¹¶ä¿å­˜{task}çš„æœ€ç»ˆæ¨¡å‹...")
            try:
                # ä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒæœ€ç»ˆæ¨¡å‹
                final_xgb_params = self.get_task_specific_xgb_params(task)
                final_xgb_model = XGBRegressor(**final_xgb_params)
                final_xgb_model.fit(X_final, y_final)
                
                final_rf_model = RandomForestRegressor(random_state=42)
                final_rf_model.fit(X_final, y_final)
                
                # ä¿å­˜æ¨¡å‹å’Œç‰¹å¾é€‰æ‹©å™¨
                self._save_ml_models(task, final_xgb_model, final_rf_model, 
                                    self.feature_selectors.get(task))
            except Exception as e:
                print(f"    âŒ ä¿å­˜{task}æœ€ç»ˆæ¨¡å‹å¤±è´¥: {e}")
        
        print("\n=== äº¤å‰éªŒè¯ç»“æœæ±‡æ€» ===")
        for task, score in mae_scores.items():
            print(f"{task}: {score:.4f}")
        
        return predictions_df, mae_scores, cv_true_values, cv_pred_values
    
    def train_torch_molecule_models(self):
        """è®­ç»ƒtorch_moleculeæ¨¡å‹ä½œä¸ºè¡¥å……"""
        if not self.use_torch_molecule:
            return None, None
        
        print("\nğŸ§  è®­ç»ƒtorch_moleculeæ¨¡å‹...")
        
        # å‡†å¤‡æ•°æ® - ä½¿ç”¨æ‰€æœ‰ä»»åŠ¡çš„æ•°æ®
        all_smiles = []
        all_labels = []
        
        # æ”¶é›†æ‰€æœ‰ä»»åŠ¡çš„æ•°æ®
        for task in self.task_names:
            task_data = self.subtables[task]
            # ä¸­ç­‰è§„æ¨¡æ¨¡å¼ä¸‹é€‚åº¦é‡‡æ ·æ•°æ®
            if self.fast_mode and len(task_data) > 5000:
                task_data = task_data.sample(n=5000, random_state=42)
                print(f"ğŸ“Š ä¸­ç­‰è§„æ¨¡æ¨¡å¼ï¼š{task}ä»»åŠ¡æ•°æ®é‡‡æ ·è‡³5000ä¸ªæ ·æœ¬")
            
            all_smiles.extend(task_data['SMILES'].tolist())
            # åˆ›å»ºå¤šä»»åŠ¡æ ‡ç­¾ï¼Œç¼ºå¤±çš„ä»»åŠ¡ç”¨NaNå¡«å……
            for _, row in task_data.iterrows():
                label_row = [np.nan] * len(self.task_names)
                task_idx = self.task_names.index(task)
                label_row[task_idx] = row[task]
                all_labels.append(label_row)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        y_all = np.array(all_labels)
        
        # åˆ†å‰²æ•°æ®
        X_train, X_val, y_train, y_val = train_test_split(
            all_smiles, y_all, test_size=0.2, random_state=42
        )
        
        # LSTMæ¨¡å‹ - ä¸­ç­‰è§„æ¨¡è®­ç»ƒæ¨¡å¼
        print("ğŸ“Š è®­ç»ƒLSTMæ¨¡å‹ï¼ˆä¸­ç­‰è§„æ¨¡æ¨¡å¼ï¼‰...")
        search_parameters_lstm = {
            "output_dim": ParameterSpec(ParameterType.INTEGER, (32, 64)),  # æ‰©å¤§æœç´¢èŒƒå›´
            "LSTMunits": ParameterSpec(ParameterType.INTEGER, (128, 256)),   # æ‰©å¤§æœç´¢èŒƒå›´
            "learning_rate": ParameterSpec(ParameterType.LOG_FLOAT, (1e-4, 1e-2)),  # æ‰©å¤§æœç´¢èŒƒå›´
        }
        
        lstm = LSTMMolecularPredictor(
            device=self.device,  # å¯ç”¨GPUåŠ é€Ÿ
            task_type="regression",
            num_task=5,
            batch_size=640,   # å¢å¤§batch_sizeä»¥å‡å°‘è®­ç»ƒæ—¶é—´
            epochs=40,        # å‡å°‘epochsçº¦20%
            verbose=True,
            patience=8        # å‡å°‘patienceä»¥æ›´æ—©åœæ­¢
        )
        
        print(f"ğŸ“± LSTMæ¨¡å‹è®¾å¤‡: {self.device}")
        
        lstm.autofit(
            X_train=X_train,
            y_train=y_train,
            X_val=X_val,
            y_val=y_val,
            search_parameters=search_parameters_lstm,
            n_trials=4       # å‡å°‘è¯•éªŒæ¬¡æ•°ä»¥èŠ‚çœæ—¶é—´
        )
        
        # GNNæ¨¡å‹ - ä¸­ç­‰è§„æ¨¡è®­ç»ƒæ¨¡å¼
        print("ğŸ•¸ï¸ è®­ç»ƒGNNæ¨¡å‹ï¼ˆä¸­ç­‰è§„æ¨¡æ¨¡å¼ï¼‰...")
        search_parameters_gnn = {
            'num_layer': ParameterSpec(
                param_type=ParameterType.INTEGER,
                value_range=(3, 6)  # æ‰©å¤§å±‚æ•°èŒƒå›´
            ),
            'hidden_size': ParameterSpec(
                param_type=ParameterType.INTEGER,
                value_range=(256, 512)  # æ‰©å¤§éšè—å±‚å¤§å°èŒƒå›´
            ),
            'learning_rate': ParameterSpec(
                param_type=ParameterType.LOG_FLOAT,
                value_range=(1e-4, 1e-2)  # æ‰©å¤§å­¦ä¹ ç‡èŒƒå›´
            ),
        }
        
        gnn = GNNMolecularPredictor(
            device=self.device,  # å¯ç”¨GPUåŠ é€Ÿ
            task_type="regression",
            num_task=5,
            batch_size=640,   # å¢å¤§batch_sizeä»¥å‡å°‘è®­ç»ƒæ—¶é—´
            epochs=40,        # å‡å°‘epochsçº¦20%
            verbose=True,
            patience=8        # å‡å°‘patienceä»¥æ›´æ—©åœæ­¢
        )
        
        print(f"ğŸ•¸ï¸ GNNæ¨¡å‹è®¾å¤‡: {self.device}")
        
        gnn.autofit(
            X_train=X_train,
            y_train=y_train,
            X_val=X_val,
            y_val=y_val,
            search_parameters=search_parameters_gnn,
            n_trials=4       # å‡å°‘è¯•éªŒæ¬¡æ•°ä»¥èŠ‚çœæ—¶é—´
        )
        
        # GPUå†…å­˜æ¸…ç†
        if self.device.type == 'cuda':
            import torch
            torch.cuda.empty_cache()
            print("ğŸ§¹ GPUå†…å­˜å·²æ¸…ç†")
        
        return {'lstm': lstm, 'gnn': gnn}, None
    
    def create_final_submission(self, ml_predictions, torch_predictions=None):
        """åˆ›å»ºæœ€ç»ˆæäº¤æ–‡ä»¶"""
        print("\nğŸ“ åˆ›å»ºæœ€ç»ˆæäº¤æ–‡ä»¶...")
        
        if torch_predictions is not None and self.use_torch_molecule:
            # è·å–torch_moleculeé¢„æµ‹
            test_smiles = self.test_df['SMILES'].tolist()
            lstm_preds = torch_predictions['lstm'].predict(test_smiles)['prediction']
            gnn_preds = torch_predictions['gnn'].predict(test_smiles)['prediction']
            torch_preds = (lstm_preds + gnn_preds) / 2
            
            # åˆ›å»ºtorché¢„æµ‹DataFrame
            torch_df = pd.DataFrame(torch_preds, columns=self.task_names)
            torch_df['id'] = self.test_df['id']
            
            # é›†æˆMLå’Œtorché¢„æµ‹ï¼ˆæƒé‡ï¼šML 0.7, torch 0.3ï¼‰
            final_predictions = ml_predictions.copy()
            for task in self.task_names:
                final_predictions[task] = (
                    0.7 * ml_predictions[task] + 
                    0.3 * torch_df[task]
                )
        else:
            final_predictions = ml_predictions.copy()
        
        # ä¿å­˜æäº¤æ–‡ä»¶åˆ°å¤šä¸ªä½ç½®ç¡®ä¿èƒ½æ‰¾åˆ°
        import os
        
        # æ–¹æ¡ˆ1: å½“å‰ç›®å½•
        submission_file = 'submission.csv'
        final_predictions.to_csv(submission_file, index=False)
        print(f"âœ… æäº¤æ–‡ä»¶å·²ä¿å­˜åˆ°å½“å‰ç›®å½•: {os.path.abspath(submission_file)}")
        
        # æ–¹æ¡ˆ2: çˆ¶ç›®å½•ï¼ˆKaggleæ ¹ç›®å½•ï¼‰
        parent_submission = os.path.join('..', 'submission.csv')
        final_predictions.to_csv(parent_submission, index=False)
        print(f"âœ… æäº¤æ–‡ä»¶å·²ä¿å­˜åˆ°çˆ¶ç›®å½•: {os.path.abspath(parent_submission)}")
        
        # æ–¹æ¡ˆ3: ç»å¯¹è·¯å¾„åˆ°Kaggleæ ¹ç›®å½•
        kaggle_root = r'f:\PythonStudy\Kaggle\submission.csv'
        final_predictions.to_csv(kaggle_root, index=False)
        print(f"âœ… æäº¤æ–‡ä»¶å·²ä¿å­˜åˆ°Kaggleæ ¹ç›®å½•: {kaggle_root}")
        
        print(f"ğŸ“Š é¢„æµ‹æ ·æœ¬æ•°: {len(final_predictions)}")
        print("ğŸ“ æ–‡ä»¶å·²ä¿å­˜åˆ°3ä¸ªä½ç½®ï¼Œè¯·ä»ä»»æ„ä½ç½®ä¸Šä¼ åˆ°Kaggle")
        
        return final_predictions
    
    def run_complete_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„è§£å†³æ–¹æ¡ˆæµç¨‹"""
        print("ğŸš€ å¼€å§‹è¿è¡Œç»ˆæè§£å†³æ–¹æ¡ˆæµç¨‹...")
        
        # 1. åŠ è½½å’Œå¤„ç†æ•°æ®
        self.load_and_split_data()
        
        # 2. è®­ç»ƒä»»åŠ¡ç‰¹å®šçš„ä¼ ç»ŸMLæ¨¡å‹
        ml_predictions, mae_scores, cv_true_values, cv_pred_values = self.train_task_specific_models()
        
        # 3. è®­ç»ƒtorch_moleculeæ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨æ·±åº¦å­¦ä¹ ä¸”å¯ç”¨ï¼‰
        torch_models = None
        if self.use_deep_learning:
            print("ğŸ§  æ·±åº¦å­¦ä¹ å·²å¯ç”¨ï¼Œå¼€å§‹è®­ç»ƒtorch_moleculeæ¨¡å‹...")
            torch_models, _ = self.train_torch_molecule_models()
        else:
            print("ğŸ§  æ·±åº¦å­¦ä¹ å·²ç¦ç”¨ï¼Œè·³è¿‡torch_moleculeæ¨¡å‹è®­ç»ƒ")
        
        # 4. åˆ›å»ºæœ€ç»ˆæäº¤
        final_predictions = self.create_final_submission(ml_predictions, torch_models)
        
        # 5. è®¡ç®—ç«èµ›æ ‡å‡†çš„åŠ æƒMAEï¼ˆåŸºäºå®é™…äº¤å‰éªŒè¯ç»“æœï¼‰
        # ä½¿ç”¨å®é™…çš„äº¤å‰éªŒè¯çœŸå®å€¼å’Œé¢„æµ‹å€¼
        if cv_true_values and cv_pred_values:
            competition_wmae, competition_weights = calculate_competition_wmae(
                cv_true_values, cv_pred_values, self.task_names
            )
        else:
            competition_wmae = 999.0
            competition_weights = {}

        # ä¹Ÿè®¡ç®—ç®€å•åŠ æƒMAEç”¨äºå¯¹æ¯”
        simple_weighted_mae = sum(mae_scores[task] * self.task_weights[task] for task in self.task_names)

        print(f"\nğŸ† ç»ˆæè§£å†³æ–¹æ¡ˆå®Œæˆ!")
        print(f"ğŸ“Š ç«èµ›æ ‡å‡†wMAEä¼°è®¡: {competition_wmae:.4f}")
        print(f"ğŸ“Š ç®€å•åŠ æƒMAEä¼°è®¡: {simple_weighted_mae:.4f}")
        print("\nå„ä»»åŠ¡è¡¨ç°:")
        for task in self.task_names:
            comp_weight = competition_weights.get(task, 0.0)
            simple_weight = self.task_weights[task]
            print(f"  {task}: MAE = {mae_scores[task]:.4f}")
            print(f"    ç«èµ›æƒé‡ = {comp_weight:.4f}, ç®€å•æƒé‡ = {simple_weight:.4f}")
        
        # æœ€ç»ˆGPUå†…å­˜æ¸…ç†
        if hasattr(self, 'device') and self.device.type == 'cuda':
            import torch
            torch.cuda.empty_cache()
            final_memory = torch.cuda.memory_allocated() / 1024**3
            print(f"\nğŸ§¹ æœ€ç»ˆGPUå†…å­˜æ¸…ç†å®Œæˆï¼Œå½“å‰ä½¿ç”¨: {final_memory:.2f} GB")
        
        return competition_wmae

    # ğŸ¯ æ–°å¢æ•°æ®å¤„ç†å‡½æ•° - æ·±åº¦é›†æˆæœ€ä¼˜æ•°æ®é›†

    def _process_polyinfo_tg_data(self, df_or_path):
        """å¤„ç†PolyInfoæƒå¨Tgæ•°æ®é›†ï¼ˆåŒ…å«èšåˆç‰©åˆ†ç±»ä¿¡æ¯ï¼‰"""
        try:
            print("      ğŸ›ï¸ å¤„ç†PolyInfoæƒå¨Tgæ•°æ®é›†...")

            # å¦‚æœä¼ å…¥çš„æ˜¯è·¯å¾„ï¼Œè¯»å–æ–‡ä»¶
            if isinstance(df_or_path, str):
                try:
                    df = pd.read_csv(df_or_path)
                    print(f"      âœ… æˆåŠŸè¯»å–PolyInfoæ•°æ®é›†: {len(df)}ä¸ªæ ·æœ¬")
                except Exception as e:
                    print(f"      âŒ æ— æ³•è¯»å–PolyInfoæ•°æ®æ–‡ä»¶: {e}")
                    return pd.DataFrame(columns=['SMILES', 'Tg'])
            else:
                df = df_or_path

            # éªŒè¯å¿…è¦åˆ—
            required_cols = ['SMILES', 'Tg']
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                print(f"      âŒ ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
                return pd.DataFrame(columns=['SMILES', 'Tg'])

            # åŸºæœ¬æ•°æ®æ¸…æ´—
            df_clean = df.copy()

            # æ¸…æ´—SMILES
            df_clean['SMILES'] = df_clean['SMILES'].apply(self.clean_and_validate_smiles)

            # ç§»é™¤æ— æ•ˆæ•°æ®
            before_clean = len(df_clean)
            df_clean = df_clean.dropna(subset=['SMILES', 'Tg'])

            # æ•°å€¼ç±»å‹è½¬æ¢
            df_clean['Tg'] = pd.to_numeric(df_clean['Tg'], errors='coerce')
            df_clean = df_clean.dropna(subset=['Tg'])

            # Tgåˆç†æ€§æ£€æŸ¥ï¼ˆ-200Â°Cåˆ°600Â°Cï¼‰
            df_clean = df_clean[(df_clean['Tg'] >= -200) & (df_clean['Tg'] <= 600)]

            after_clean = len(df_clean)
            print(f"      âœ… PolyInfoæ•°æ®æ¸…æ´—: {before_clean} â†’ {after_clean} æ ·æœ¬")

            # ä¿ç•™èšåˆç‰©åˆ†ç±»ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            result_cols = ['SMILES', 'Tg']
            if 'Polymer Class' in df.columns:
                result_cols.append('Polymer Class')
                print(f"      ğŸ“Š ä¿ç•™èšåˆç‰©åˆ†ç±»ä¿¡æ¯: {df_clean['Polymer Class'].nunique()} ä¸ªç±»åˆ«")
            if 'PID' in df.columns:
                result_cols.append('PID')
                print(f"      ğŸ†” ä¿ç•™èšåˆç‰©IDä¿¡æ¯")

            return df_clean[result_cols]

        except Exception as e:
            print(f"      âŒ å¤„ç†PolyInfoæ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame(columns=['SMILES', 'Tg'])

    def _process_github_conjugated_polymers(self, df_or_path):
        """å¤„ç†Githubé¡¹ç›®çš„32ä¸ªé«˜è´¨é‡å…±è½­èšåˆç‰©æ•°æ®"""
        try:
            print("      ğŸ¥‡ å¤„ç†Githubé«˜è´¨é‡å…±è½­èšåˆç‰©æ•°æ®...")

            # å¦‚æœä¼ å…¥çš„æ˜¯è·¯å¾„ï¼Œè¯»å–æ–‡ä»¶
            if isinstance(df_or_path, str):
                try:
                    # ä½¿ç”¨æ‰‹åŠ¨è¯»å–æ–¹å¼ç¡®ä¿æ­£ç¡®è§£æåˆ¶è¡¨ç¬¦åˆ†éš”æ–‡ä»¶
                    with open(df_or_path, 'r', encoding='utf-8') as f:
                        lines = f.readlines()

                    # è§£æå¤´éƒ¨
                    header = lines[0].strip().split('\t')

                    # è§£ææ•°æ®è¡Œ
                    data = []
                    for line in lines[1:]:
                        parts = line.strip().split('\t')
                        if len(parts) == len(header):
                            data.append(parts)

                    df = pd.DataFrame(data, columns=header)
                    print(f"      âœ… æˆåŠŸè¯»å–Githubå…±è½­èšåˆç‰©æ•°æ®: {len(df)}ä¸ªæ ·æœ¬")
                except Exception as e:
                    print(f"      âŒ æ— æ³•è¯»å–Githubæ•°æ®æ–‡ä»¶: {e}")
                    return pd.DataFrame(columns=['SMILES', 'Tg'])
            else:
                df = df_or_path

            print(f"      ğŸ“Š åŸå§‹Githubæ•°æ®: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
            print(f"      ğŸ“‹ åˆ—å: {list(df.columns)}")

            # æ¸…ç†åˆ—åï¼ˆç§»é™¤å¯èƒ½çš„ç©ºç™½å­—ç¬¦ï¼‰
            df.columns = df.columns.str.strip()

            # æ ‡å‡†åŒ–åˆ—å
            df_clean = pd.DataFrame()

            # å¤„ç†Tgåˆ—
            if 'Tg' in df.columns:
                df_clean['Tg'] = pd.to_numeric(df['Tg'], errors='coerce')
            else:
                print(f"      âŒ ç¼ºå°‘Tgåˆ—")
                return pd.DataFrame(columns=['SMILES', 'Tg'])

            # å¤„ç†SMILESåˆ—
            if 'Smiles' in df.columns:
                df_clean['SMILES'] = df['Smiles']
            elif 'SMILES' in df.columns:
                df_clean['SMILES'] = df['SMILES']
            else:
                print(f"      âŒ ç¼ºå°‘SMILESåˆ—")
                return pd.DataFrame(columns=['SMILES', 'Tg'])

            # æ¸…æ´—SMILES
            df_clean['SMILES'] = df_clean['SMILES'].apply(self.clean_and_validate_smiles)

            # ç§»é™¤æ— æ•ˆæ•°æ®
            before_clean = len(df_clean)
            df_clean = df_clean.dropna(subset=['SMILES', 'Tg'])
            after_clean = len(df_clean)

            print(f"      âœ… Githubæ•°æ®æ¸…æ´—: {before_clean} â†’ {after_clean} æ ·æœ¬")

            if after_clean > 0:
                print(f"      ğŸ“ˆ TgèŒƒå›´: {df_clean['Tg'].min():.1f} ~ {df_clean['Tg'].max():.1f}Â°C")
                print(f"      ğŸ“Š å¹³å‡Tg: {df_clean['Tg'].mean():.1f}Â°C")

                # æ£€æŸ¥èšåˆç‰©æ ‡è®°
                has_star = df_clean['SMILES'].str.contains(r'\*').sum()
                print(f"      ğŸ”— åŒ…å«èšåˆç‰©æ ‡è®°[*]: {has_star}/{len(df_clean)} æ ·æœ¬")

                # æ˜¾ç¤ºæ ·æœ¬é¢„è§ˆ
                print("      ğŸ” é«˜æ¸©èšåˆç‰©æ ·æœ¬é¢„è§ˆ:")
                high_temp = df_clean[df_clean['Tg'] > 100].head(3)
                for i, (_, row) in enumerate(high_temp.iterrows()):
                    print(f"        {i+1}. Tg={row['Tg']}Â°C: {row['SMILES'][:50]}...")

            return df_clean[['SMILES', 'Tg']]

        except Exception as e:
            print(f"      âŒ å¤„ç†Githubå…±è½­èšåˆç‰©æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame(columns=['SMILES', 'Tg'])


    def _process_pi1070_data(self, df_or_path):
        """å¤„ç†PI1070å¤šç‰©æ€§æ•°æ®é›†ï¼ˆ1077æ ·æœ¬ï¼Œ157ç‰¹å¾ï¼‰"""
        try:
            print("      ğŸ¥‡ å¤„ç†PI1070å¤šç‰©æ€§æ•°æ®é›†...")

            # å¦‚æœä¼ å…¥çš„æ˜¯è·¯å¾„ï¼Œè¯»å–æ–‡ä»¶
            if isinstance(df_or_path, str):
                try:
                    df = pd.read_csv(df_or_path)
                    print(f"      âœ… æˆåŠŸè¯»å–PI1070æ•°æ®é›†: {len(df)}ä¸ªæ ·æœ¬")
                except Exception as e:
                    print(f"      âŒ æ— æ³•è¯»å–PI1070æ•°æ®æ–‡ä»¶: {e}")
                    return pd.DataFrame(columns=['SMILES'])
            else:
                df = df_or_path

            print(f"      ğŸ“Š åŸå§‹PI1070æ•°æ®: {len(df)} è¡Œ, {len(df.columns)} åˆ—")

            # æ£€æŸ¥å¿…è¦åˆ—
            if 'SMILES' not in df.columns:
                print(f"      âŒ PI1070æ•°æ®ç¼ºå°‘SMILESåˆ—")
                return pd.DataFrame(columns=['SMILES'])

            # åˆ›å»ºæ ‡å‡†åŒ–æ•°æ®æ¡†
            result_df = pd.DataFrame()
            result_df['SMILES'] = df['SMILES']

            # æå–ä¸»è¦ç›®æ ‡å˜é‡
            target_mappings = {
                'density': 'Density',
                'Rg': 'Rg',
                'thermal_conductivity': 'Tc'
            }

            extracted_targets = []
            for source_col, target_col in target_mappings.items():
                if source_col in df.columns:
                    result_df[target_col] = pd.to_numeric(df[source_col], errors='coerce')
                    valid_count = result_df[target_col].notna().sum()
                    if valid_count > 0:
                        extracted_targets.append(f"{target_col}({valid_count})")
                        print(f"      ğŸ“ˆ {target_col}: {valid_count} æœ‰æ•ˆæ ·æœ¬, èŒƒå›´: {result_df[target_col].min():.3f} ~ {result_df[target_col].max():.3f}")

            # æ¸…æ´—SMILES
            result_df['SMILES'] = result_df['SMILES'].apply(self.clean_and_validate_smiles)

            # ç§»é™¤æ— æ•ˆæ•°æ®
            before_clean = len(result_df)
            result_df = result_df.dropna(subset=['SMILES'])
            after_clean = len(result_df)

            print(f"      âœ… PI1070æ•°æ®æ¸…æ´—: {before_clean} â†’ {after_clean} æ ·æœ¬")
            print(f"      ğŸ¯ æå–ç›®æ ‡: {', '.join(extracted_targets)}")

            return result_df

        except Exception as e:
            print(f"      âŒ å¤„ç†PI1070æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame(columns=['SMILES'])

    def _process_tg_density_data(self, df_or_path):
        """å¤„ç†Tg+å¯†åº¦å®éªŒæ•°æ®é›†ï¼ˆ194æ ·æœ¬ï¼‰"""
        try:
            print("      ğŸ¥ˆ å¤„ç†Tg+å¯†åº¦å®éªŒæ•°æ®é›†...")

            # å¦‚æœä¼ å…¥çš„æ˜¯è·¯å¾„ï¼Œè¯»å–æ–‡ä»¶
            if isinstance(df_or_path, str):
                try:
                    df = pd.read_csv(df_or_path)
                    print(f"      âœ… æˆåŠŸè¯»å–Tg+å¯†åº¦æ•°æ®é›†: {len(df)}ä¸ªæ ·æœ¬")
                except Exception as e:
                    print(f"      âŒ æ— æ³•è¯»å–Tg+å¯†åº¦æ•°æ®æ–‡ä»¶: {e}")
                    return pd.DataFrame(columns=['SMILES', 'Tg', 'Density'])
            else:
                df = df_or_path

            print(f"      ğŸ“Š åŸå§‹Tg+å¯†åº¦æ•°æ®: {len(df)} è¡Œ, {len(df.columns)} åˆ—")

            # æ£€æŸ¥å¿…è¦åˆ—
            required_cols = ['SMILES', 'Tg', 'Density']
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                print(f"      âŒ Tg+å¯†åº¦æ•°æ®ç¼ºå°‘åˆ—: {missing_cols}")
                return pd.DataFrame(columns=['SMILES', 'Tg', 'Density'])

            # åˆ›å»ºæ ‡å‡†åŒ–æ•°æ®æ¡†
            result_df = pd.DataFrame()
            result_df['SMILES'] = df['SMILES']
            result_df['Tg'] = pd.to_numeric(df['Tg'], errors='coerce')
            result_df['Density'] = pd.to_numeric(df['Density'], errors='coerce')

            # æ¸…æ´—SMILES
            result_df['SMILES'] = result_df['SMILES'].apply(self.clean_and_validate_smiles)

            # ç§»é™¤æ— æ•ˆæ•°æ®
            before_clean = len(result_df)
            result_df = result_df.dropna(subset=['SMILES'])

            # ç»Ÿè®¡æœ‰æ•ˆç›®æ ‡æ•°æ®
            tg_valid = result_df['Tg'].notna().sum()
            density_valid = result_df['Density'].notna().sum()

            after_clean = len(result_df)
            print(f"      âœ… Tg+å¯†åº¦æ•°æ®æ¸…æ´—: {before_clean} â†’ {after_clean} æ ·æœ¬")

            if tg_valid > 0:
                print(f"      ğŸ“ˆ Tg: {tg_valid} æœ‰æ•ˆæ ·æœ¬, èŒƒå›´: {result_df['Tg'].min():.1f} ~ {result_df['Tg'].max():.1f}Â°C")
            if density_valid > 0:
                print(f"      ğŸ“ˆ å¯†åº¦: {density_valid} æœ‰æ•ˆæ ·æœ¬, èŒƒå›´: {result_df['Density'].min():.3f} ~ {result_df['Density'].max():.3f}")

            return result_df[['SMILES', 'Tg', 'Density']]

        except Exception as e:
            print(f"      âŒ å¤„ç†Tg+å¯†åº¦æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame(columns=['SMILES', 'Tg', 'Density'])

    def apply_frequency_based_feature_selection(self, X, feature_names=None):
        """
        åŸºäºé¢‘æ¬¡çš„ç‰¹å¾é€‰æ‹© - å€Ÿé‰´Githubé¡¹ç›®çš„æ–¹æ³•
        é€‰æ‹©åœ¨æ•°æ®é›†ä¸­å‡ºç°é¢‘æ¬¡é€‚ä¸­çš„ç‰¹å¾ï¼Œé¿å…è¿‡äºç¨€ç–æˆ–è¿‡äºå¸¸è§çš„ç‰¹å¾
        """
        try:
            print("      ğŸ” åº”ç”¨åŸºäºé¢‘æ¬¡çš„ç‰¹å¾é€‰æ‹©...")

            if feature_names is None:
                feature_names = [f'feature_{i}' for i in range(X.shape[1])]

            # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„éé›¶é¢‘æ¬¡
            feature_frequencies = np.sum(X != 0, axis=0)

            # æ›´æ–°å…¨å±€é¢‘æ¬¡ç»Ÿè®¡
            for i, freq in enumerate(feature_frequencies):
                feat_name = feature_names[i] if i < len(feature_names) else f'feature_{i}'
                if feat_name not in self.feature_frequency_stats:
                    self.feature_frequency_stats[feat_name] = 0
                self.feature_frequency_stats[feat_name] += freq

            # åº”ç”¨é¢‘æ¬¡é˜ˆå€¼è¿‡æ»¤
            # é€‰æ‹©é¢‘æ¬¡åœ¨åˆç†èŒƒå›´å†…çš„ç‰¹å¾ï¼ˆä¸å¤ªç¨€ç–ï¼Œä¹Ÿä¸å¤ªå¸¸è§ï¼‰
            min_frequency = max(1, X.shape[0] * 0.01)  # è‡³å°‘1%çš„æ ·æœ¬åŒ…å«è¯¥ç‰¹å¾
            max_frequency = min(self.frequency_threshold, X.shape[0] * 0.95)  # æœ€å¤š95%çš„æ ·æœ¬åŒ…å«è¯¥ç‰¹å¾

            selected_features = []
            for i, freq in enumerate(feature_frequencies):
                if min_frequency <= freq <= max_frequency:
                    selected_features.append(i)

            if len(selected_features) == 0:
                print(f"      âš ï¸ é¢‘æ¬¡è¿‡æ»¤åæ— ç‰¹å¾ä¿ç•™ï¼Œä½¿ç”¨æ–¹å·®è¿‡æ»¤")
                # å›é€€åˆ°æ–¹å·®è¿‡æ»¤
                variance_selector = VarianceThreshold(threshold=0.01)
                X_selected = variance_selector.fit_transform(X)
                selected_features = variance_selector.get_support(indices=True)
            else:
                X_selected = X[:, selected_features]

            print(f"      âœ… é¢‘æ¬¡ç‰¹å¾é€‰æ‹©: {X.shape[1]} â†’ {len(selected_features)} ç‰¹å¾")
            print(f"      ğŸ“Š é¢‘æ¬¡èŒƒå›´: {min_frequency:.0f} ~ {max_frequency:.0f}")

            return X_selected, selected_features

        except Exception as e:
            print(f"      âŒ é¢‘æ¬¡ç‰¹å¾é€‰æ‹©å¤±è´¥: {e}")
            return X, list(range(X.shape[1]))

def main(model_path=None):
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ å¯åŠ¨ç»ˆæè§£å†³æ–¹æ¡ˆ V7...")
    
    # ==================== é…ç½®è®¾ç½®åŒºåŸŸ ====================
    # åœ¨è¿™é‡Œç›´æ¥ä¿®æ”¹CUSTOM_MODEL_PATHæ¥æŒ‡å®šæ¨¡å‹è·¯å¾„
    # ä¾‹å¦‚: CUSTOM_MODEL_PATH = "/kaggle/input/saved-models"  # Kaggleç¯å¢ƒ
    # ä¾‹å¦‚: CUSTOM_MODEL_PATH = "C:/path/to/models"          # Windowsè·¯å¾„
    # ä¾‹å¦‚: CUSTOM_MODEL_PATH = "/path/to/models"            # Linux/Macè·¯å¾„
    # è®¾ç½®ä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„(å½“å‰ç›®å½•ä¸‹çš„saved_modelsæ–‡ä»¶å¤¹)
    
    CUSTOM_MODEL_PATH = None  # ğŸ‘ˆ åœ¨è¿™é‡Œè¾“å…¥æ‚¨çš„æ¨¡å‹è·¯å¾„
    
    # æ·±åº¦å­¦ä¹ å¼€å…³è®¾ç½®
    # True: å¯ç”¨æ·±åº¦å­¦ä¹ (torch_moleculeæ¨¡å‹)ï¼Œè®­ç»ƒæ—¶é—´æ›´é•¿ä½†å¯èƒ½æ•ˆæœæ›´å¥½
    # False: ç¦ç”¨æ·±åº¦å­¦ä¹ ï¼Œä»…ä½¿ç”¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«
    USE_DEEP_LEARNING = False  # ğŸ‘ˆ åœ¨è¿™é‡Œè®¾ç½®æ˜¯å¦å¯ç”¨æ·±åº¦å­¦ä¹ 
    
    # ========================================================
    
    # ä¼˜å…ˆä½¿ç”¨ä»£ç ä¸­è®¾ç½®çš„è·¯å¾„ï¼Œå…¶æ¬¡ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
    final_model_path = CUSTOM_MODEL_PATH or model_path
    
    if final_model_path:
        print(f"ğŸ“ ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„: {final_model_path}")
    else:
        print("ğŸ“ ä½¿ç”¨é»˜è®¤æ¨¡å‹è·¯å¾„: ./saved_models")
    
    # åˆ›å»ºè§£å†³æ–¹æ¡ˆå®ä¾‹
    solution = UltimateSolution(
        use_torch_molecule=True,
        fast_mode=True,
        model_path=final_model_path,
        use_deep_learning=USE_DEEP_LEARNING,
        use_saved_models=False  # ğŸš« ç¦ç”¨å·²ä¿å­˜æ¨¡å‹ï¼Œå¼ºåˆ¶é‡æ–°è®­ç»ƒ
    )
    
    if solution.use_deep_learning:
        print("ğŸ“Š æ³¨æ„ï¼šæ·±åº¦å­¦ä¹ å·²å¯ç”¨ï¼Œå°†ä½¿ç”¨torch_moleculeæ¨¡å‹")
        print("ğŸ”§ è®­ç»ƒå‚æ•°è®¾ç½®ï¼šepochs=50, trials=10, batch_size=512, æ‰©å¤§å‚æ•°æœç´¢èŒƒå›´")
    else:
        print("ğŸ“Š æ³¨æ„ï¼šæ·±åº¦å­¦ä¹ å·²ç¦ç”¨ï¼Œä»…ä½¿ç”¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹")
    print(f"ğŸ–¥ï¸ è®¡ç®—è®¾å¤‡: {solution.device}")
    
    if solution.device.type == 'cuda':
        import torch
        print(f"ğŸ’¾ GPUå†…å­˜çŠ¶æ€: {torch.cuda.memory_allocated() / 1024**3:.2f} GB / {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # è¿è¡Œå®Œæ•´æµç¨‹
    final_score = solution.run_complete_pipeline()
    
    return final_score

if __name__ == "__main__":
    import sys
    print("ğŸš€ ç¨‹åºå¼€å§‹æ‰§è¡Œ...")
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    model_path = None
    if len(sys.argv) > 1:
        model_path = sys.argv[1]
        print(f"ğŸ“ ä»å‘½ä»¤è¡Œå‚æ•°è·å–æ¨¡å‹è·¯å¾„: {model_path}")
    
    try:
        final_score = main(model_path=model_path)
        print(f"\nğŸ ç¨‹åºæ‰§è¡Œå®Œæˆï¼Œæœ€ç»ˆå¾—åˆ†: {final_score:.4f}")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
